{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and settings\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import MDAnalysis as mda\n",
    "import seaborn as sns\n",
    "import nglview as nv                            # for visualisation\n",
    "from MDAnalysis.analysis.align import alignto   # for aligning structures\n",
    "from MDAnalysis.analysis.pca import PCA         # for PCA\n",
    "from Bio.PDB import PDBParser\n",
    "from Bio.PDB.DSSP import DSSP                   # for secondary structure selection\n",
    "from Bio.PDB.SASA import ShrakeRupley           # for SASA calculation\n",
    "from IPython.display import display             # for data frame display\n",
    "from multiprocessing import Pool                # for multiprocessing\n",
    "from tqdm import tqdm                           # for progress bars\n",
    "\n",
    "# surpress warnings\n",
    "warnings.filterwarnings(action='ignore', module='matplotlib')\n",
    "warnings.filterwarnings(action='ignore', module='mdanalysis')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# pandas settings\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# base directories\n",
    "base_directory = \"/biggin/b212/bioc1781/Projects/CTNS/human/monomer/red-msa/with-dropout/\" #\"/biggin/b212/bioc1781/Projects/KDELR/red-msa/\"  #\"/biggin/b212/bioc1781/Projects/CTNS/human/monomer/red-msa/with-dropout/\"\n",
    "structure_directory = base_directory + \"ensemble/\" # directory with the AF2 ensemble, cannot be the same as base directory\n",
    "\n",
    "# settings\n",
    "selection_for_writeout  = \"protein and name CA\"  # useful if you want all the outputs to have a specific selection\n",
    "conserved_residues      = \"resid 162 or resid 281 or resid 142 or resid 143 or resid 346 or resid 345 or resid 305 or resid 280 or resid 138 or resid 211 or resid 208 or resid 273 or resid 173 or resid 335 or resid 205 or resid 332 or resid 176 \" #\"resid 205 or resid 305 or resid 346 or resid 169 or resid 308 or resid 339 or resid 260 or resid 158 or resid 338 or resid 177 or resid 288 or resid 222 or resid 139 or resid 141 or resid 298 or resid 182 or resid 280 or resid 335 or resid 142 or resid 138 or resid 170 or resid 208 or resid 173 or resid 134\"        #'resid 205 or resid 305 or resid 346 or resid 169 or resid 308 or resid 339 or resid 260 or resid 158 or resid 338 or resid 177 or resid 288 or resid 222 or resid 139 or resid 141 or resid 298 or resid 182 or resid 280 or resid 335 or resid 142 or resid 138 or resid 170 or resid 208 or resid 173 or resid 134'\n",
    "excluded_residues       = \"\"        # resid 134-149\n",
    "resid_offset    = 115 #0 #115            # first resID in the reference structures if the chain does not start from 1\n",
    "# structure filtering   \n",
    "thresh_rmsd     = 10 #6             # threshold for discarding structures based on RMSD\n",
    "thresh_pLDDT    = 90                # threshold for discarding structures based on pLDDT\n",
    "thresh_sasa_coeff = 100             # multiple of highest sasa of the reference structure above which structures are discarded\n",
    "#diffmat_thresh  = 0                # threshold for ignoring atoms in RMSD calculations based on how much they differ between the reference structures\n",
    "# analysis  \n",
    "num_processes   = 12                # CPU cores to use for multiprocessing\n",
    "n_pcs           = 3                 # number of principal components to keep in PCA\n",
    "# monte carlo   \n",
    "mc_temp         = 500               # monte carlo temperature\n",
    "mc_wf_sasa      = 0                 # monte carlo energy function weight factor for SASA (check if this should be -ve, depends on order of subtraction and matrix indexing)\n",
    "mc_n_runs       = 1                 # number of monte carlo runs\n",
    "mc_n_bins       = 24                # so mc_n_bins is the total number of bins (because of reference structures and zero indexing)\n",
    "collective_variable = 'lumen_helix_bundle_separation'         # collective_variable to bin for path finding\n",
    "start_from_endstates = False\n",
    "# for plotting\n",
    "plot_variable_1     = 'PC1'   \n",
    "plot_variable_2     = 'PC2'\n",
    "plot_variable_3     = 'sasa'\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_processes)\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# reference structures\n",
    "use_reference_structures = True\n",
    "\n",
    "if use_reference_structures:\n",
    "\n",
    "    outward_open_pdb = base_directory + \"8DKE_cytosol_noNTD_hydrogens.pdb\"\n",
    "    inward_open_pdb  = base_directory + \"8DKI_lumen_noNTD_hydrogens.pdb\"\n",
    "\n",
    "    # universes for reference structures\n",
    "    u_outward_open = mda.Universe(outward_open_pdb, outward_open_pdb)\n",
    "    u_inward_open  = mda.Universe(inward_open_pdb, inward_open_pdb)\n",
    "\n",
    "    # align reference structures to eachother\n",
    "    \n",
    "    alignto(u_inward_open, u_outward_open, select='all', weights=\"mass\")\n",
    "\n",
    "    # write reindexed pdbs of reference structures for inclusion in the ensemble\n",
    "    u_outward_open.atoms.residues.resids -= resid_offset\n",
    "    u_outward_open.atoms.write(structure_directory + \"ref_outward.pdb\")\n",
    "    u_outward_open.atoms.residues.resids += resid_offset\n",
    "    #\n",
    "    u_inward_open.atoms.residues.resids -= resid_offset\n",
    "    u_inward_open.atoms.write(structure_directory + \"ref_inward.pdb\")\n",
    "    u_inward_open.atoms.residues.resids += resid_offset\n",
    "\n",
    "else:\n",
    "    outward_open_pdb = None\n",
    "    inward_open_pdb  = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve structures\n",
    "\n",
    "# user specific - do whatever you need here to get a dictionary of structures and their associated pLDDT values\n",
    "\n",
    "# # ONLY NEED TO RUN ONCE \n",
    "# # copy structures from outputs to ensemble\n",
    "# msa_depths = ['32-64', '64-128', '128-256', '256-512', '512-1024']\n",
    "# \n",
    "# for msa in msa_depths:\n",
    "#     structures_msa = [f for f in os.listdir(base_directory + 'output_' + msa) if f.endswith('.pdb')]\n",
    "#     for structure in structures_msa:\n",
    "#         # copy all the pdb files with \"_relaxed_\" in the name to the structure directory\n",
    "#         if \"_relaxed_\" in structure:\n",
    "#             # get the rank of the strcuture (last thing before the file extension)\n",
    "#             rank = 'rank_' + structure.split('_')[structure.split('_').index('rank')+1]\n",
    "#             os.system('cp ' + base_directory + 'output_' + msa + '/' + structure + ' ' + structure_directory + '/msa-' + msa + '_' + rank + '.pdb')\n",
    "# \n",
    "# # get list of all files in structure directory directory with the pdb extension\n",
    "# structures = [f for f in os.listdir(structure_directory) if f.endswith('.pdb')]\n",
    "# structures.sort()\n",
    "\n",
    "# associate structures with pLDDT values\n",
    "structure_and_pLDDT = {}\n",
    "\n",
    "msa_depths = ['8-16', '16-32', '32-64', '64-128', '128-256', '256-512', '512-1024']\n",
    "for msa in msa_depths:\n",
    "    structures_msa = [f for f in os.listdir(structure_directory) if f.endswith('.pdb') and msa in f and str(msa) in f]\n",
    "    log_file = structure_directory + msa + \"_log.txt\"   # look up the corresponding log file\n",
    "    for structure in structures_msa:\n",
    "        rank = 'rank_' + structure.split('_')[structure.split('_').index('rank')+1] # get the rank of the structure\n",
    "        rank = rank.split('.')[0]           # remove the file extension\n",
    "        with open(log_file, \"r\") as file:\n",
    "            for line in file:               # match the rank with the pLDDT value\n",
    "                if rank in line:\n",
    "                    pLDDT = line.split()[3].replace('pLDDT=','')\n",
    "                    structure_and_pLDDT[structure] = float(pLDDT)\n",
    "                    break\n",
    "\n",
    "print('Structures:', len(structure_and_pLDDT))\n",
    "\n",
    "# if no reference structures, chose 2 hightest pLDDT structures as references\n",
    "if not use_reference_structures:\n",
    "    # get the 2 structures with the highest pLDDT values\n",
    "    ref_structures = sorted(structure_and_pLDDT, key=structure_and_pLDDT.get, reverse=True)[:2]\n",
    "    print('Reference structures:', ref_structures)\n",
    "    # declare the appropriate variable (the name of the structure file)\n",
    "    outward_open_pdb = structure_directory + ref_structures[0]\n",
    "    inward_open_pdb  = structure_directory + ref_structures[1]\n",
    "    # universes for reference structures\n",
    "    u_outward_open = mda.Universe(outward_open_pdb, outward_open_pdb)\n",
    "    u_inward_open  = mda.Universe(inward_open_pdb, inward_open_pdb)\n",
    "\n",
    "    # align reference structures to eachother\n",
    "    alignto(u_inward_open, u_outward_open, select='protein', weights=\"mass\")\n",
    "\n",
    "    # write reindexed pdbs of reference structures for inclusion in the ensemble\n",
    "    #u_outward_open.atoms.residues.resids -= resid_offset\n",
    "    #u_outward_open.atoms.write(structure_directory + \"ref_outward.pdb\")\n",
    "    #u_outward_open.atoms.residues.resids += resid_offset\n",
    "    #\n",
    "    #u_inward_open.atoms.residues.resids -= resid_offset\n",
    "    #u_inward_open.atoms.write(structure_directory + \"ref_inward.pdb\")\n",
    "    #u_inward_open.atoms.residues.resids += resid_offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic \"clever\" selection definer\n",
    "\n",
    "# temporary universe\n",
    "u = u_outward_open\n",
    "\n",
    "# identify regions of secondary structure\n",
    "p = PDBParser()\n",
    "structure = p.get_structure('reference', outward_open_pdb)\n",
    "model = structure[0]\n",
    "dssp = DSSP(model, outward_open_pdb, dssp='mkdssp')\n",
    "\n",
    "helices = []\n",
    "sheets  = []\n",
    "loops   = []\n",
    "\n",
    "# get secondary structure labels for resIDs\n",
    "for key in dssp.keys():\n",
    "    if dssp[key][2] == 'H' or dssp[key][2] == 'G' or dssp[key][2] == 'I':\n",
    "        helices.append(key[1][1])\n",
    "    elif dssp[key][2] == 'E':\n",
    "        sheets.append(key[1][1])\n",
    "    elif dssp[key][2] == 'T' or dssp[key][2] == 'S':\n",
    "        loops.append(key[1][1])\n",
    "    \n",
    "helices = sorted(list(set(helices)))\n",
    "sheets = sorted(list(set(sheets)))\n",
    "loops = sorted(list(set(loops)))\n",
    "\n",
    "helices_contiguous = []\n",
    "sheets_contiguous  = []\n",
    "loops_contiguous   = []\n",
    "\n",
    "# get contiguous regions of secondary structure\n",
    "for i in range(len(helices)):\n",
    "    if i == 0:\n",
    "        helices_contiguous.append([helices[i]])\n",
    "    elif helices[i] == helices[i-1] + 1:\n",
    "        helices_contiguous[-1].append(helices[i])\n",
    "    else:\n",
    "        helices_contiguous.append([helices[i]])\n",
    "\n",
    "for i in range(len(sheets)):\n",
    "    if i == 0:\n",
    "        sheets_contiguous.append([sheets[i]])\n",
    "    elif sheets[i] == sheets[i-1] + 1:\n",
    "        sheets_contiguous[-1].append(sheets[i])\n",
    "    else:\n",
    "        sheets_contiguous.append([sheets[i]])\n",
    "\n",
    "for i in range(len(loops)):\n",
    "    if i == 0:\n",
    "        loops_contiguous.append([loops[i]])\n",
    "    elif loops[i] == loops[i-1] + 1:\n",
    "        loops_contiguous[-1].append(loops[i])\n",
    "    else:\n",
    "        loops_contiguous.append([loops[i]])\n",
    "\n",
    "selection_helices = []\n",
    "selection_sheets  = []\n",
    "selection_loops   = []\n",
    "\n",
    "# make mdanalysis selections corresponding to these regions\n",
    "for i in range(len(helices_contiguous)):\n",
    "    selection_helices.append('(resid %s-%s)' % (helices_contiguous[i][0], helices_contiguous[i][-1]))\n",
    "for i in range(len(sheets_contiguous)):\n",
    "    selection_sheets.append('(resid %s-%s)' % (sheets_contiguous[i][0], sheets_contiguous[i][-1]))\n",
    "for i in range(len(loops_contiguous)):\n",
    "    selection_loops.append('(resid %s-%s)' % (loops_contiguous[i][0], loops_contiguous[i][-1]))\n",
    "\n",
    "# conserved residues\n",
    "selection_conserved_residues = '((' + conserved_residues + ') and (name CA or name CG or name CZ* or name NZ))' \n",
    "\n",
    "# format selections\n",
    "selection_helices = ','.join(selection_helices)\n",
    "selection_helices = selection_helices.replace(',', ' or ')\n",
    "selection_sheets = ','.join(selection_sheets)\n",
    "selection_sheets = selection_sheets.replace(',', ' or ')\n",
    "selection_loops = ','.join(selection_loops)\n",
    "selection_loops = selection_loops.replace(',', ' or ')\n",
    "\n",
    "endstates = {}\n",
    "\n",
    "resids_from_ca_dist_diffmat = []\n",
    "\n",
    "## get the resid of the residues that differ by more than the threshold in absolute terms\n",
    "#above_thresh = np.where(abs(ca_dist_difference_matrix) >= diffmat_thresh)\n",
    "#for i in range(len(above_thresh[0])):\n",
    "#    resids_from_ca_dist_diffmat.append(u.atoms[above_thresh[0][i]].resid)\n",
    "#resids_from_ca_dist_diffmat = list(set(resids_from_ca_dist_diffmat))    # get unique residues\n",
    "\n",
    "# make a selection token for these residues\n",
    "selection_from_ca_dist_diffmat = []\n",
    "for i in range(len(resids_from_ca_dist_diffmat)):\n",
    "    selection_from_ca_dist_diffmat.append('resid %s' % resids_from_ca_dist_diffmat[i])\n",
    "selection_from_ca_dist_diffmat = ','.join(selection_from_ca_dist_diffmat)\n",
    "selection_from_ca_dist_diffmat = selection_from_ca_dist_diffmat.replace(',', ' or ')\n",
    "\n",
    "# final rmsd_selection for analysis\n",
    "rmsd_selection = '( ( (' + selection_helices + ') and name CA ) or ( (' + selection_loops + ') and name CA ) )' # or' + selection_conserved_residues #+ ' and not (resid 116-120 or resid 356-367)'\n",
    "if conserved_residues != \"\":\n",
    "    rmsd_selection += ' or ( (' + selection_helices + ') and' + selection_conserved_residues + ')'\n",
    "if excluded_residues != \"\":\n",
    "    rmsd_selection += ' and not (' + excluded_residues + ')'\n",
    "\n",
    "print(rmsd_selection)\n",
    "\n",
    "# write a pdb of the selection\n",
    "# select the rmsd_selection atoms in the outward open structure\n",
    "check = u.select_atoms(rmsd_selection)\n",
    "# write out the selection\n",
    "check.write(base_directory + 'rmsd_selection.pdb')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make main pandas dataframe and filter data\n",
    "ensemble_df = pd.DataFrame(index=structure_and_pLDDT.keys())\n",
    "# rename the index column to structure\n",
    "ensemble_df.index.names = ['structure']\n",
    "\n",
    "pLDDT_scores = {}\n",
    "plddt_df = pd.DataFrame(list(structure_and_pLDDT.items()), columns=['structure', 'pDDLDT']) # Convert the dictionary to a DataFrame\n",
    "\n",
    "# add entries to dictionary for the reference structures with pLDDT scores of 100\n",
    "if use_reference_structures:\n",
    "    plddt_df = pd.concat([plddt_df, pd.DataFrame({'structure': 'ref_outward.pdb', 'pDDLDT': 100}, index=[0])], ignore_index=True)   # append is deprecated, use concat instead\n",
    "    plddt_df = pd.concat([plddt_df, pd.DataFrame({'structure': 'ref_inward.pdb', 'pDDLDT': 100}, index=[0])], ignore_index=True)\n",
    "\n",
    "# Merge the two DataFrames on the structure column\n",
    "ensemble_df = pd.merge(ensemble_df, plddt_df, on='structure')\n",
    "\n",
    "# filter by pLDDT\n",
    "ensemble_df = ensemble_df[ensemble_df['pDDLDT'] > thresh_pLDDT]      # discard < thresh_pLDDT\n",
    "structures = list(ensemble_df['structure'])                          # apply the filter to the list of structure names\n",
    "\n",
    "display(ensemble_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PCA on the ensemble\n",
    "\n",
    "# write a multistate pdb file for the whole ensemble so MDA will interpret it as a trajectory\n",
    "with mda.Writer(base_directory + 'ensemble.pdb', u.atoms.n_atoms) as W:\n",
    "    for structure in ensemble_df['structure']:\n",
    "        u = mda.Universe(structure_directory + structure, \n",
    "                         structure_directory + structure)\n",
    "        # renumber for consistency and to match selection token   s     \n",
    "        u.atoms.residues.resids += resid_offset\n",
    "        u.atoms.segments.segids = 'A'\n",
    "        u.atoms.chainIDs = 'A'\n",
    "        W.write(u.select_atoms(rmsd_selection))\n",
    " \n",
    "# make a universe containing all the structures\n",
    "u = mda.Universe(base_directory + \"ensemble.pdb\")\n",
    "\n",
    "# align the ensemble to the selection token\n",
    "aligner = mda.analysis.align.AlignTraj(u, u, select=rmsd_selection, in_memory=True).run()\n",
    "\n",
    "# perform principal component analysis\n",
    "pc = PCA(u, select=rmsd_selection, align=True, mean=None, n_components=None).run()\n",
    "\n",
    "# project coorindates onto the principal components\n",
    "pc_projection = pc.transform(u.select_atoms(rmsd_selection), n_components=n_pcs)\n",
    "\n",
    "# make a dataframe to store the principal components\n",
    "pca_df = pd.DataFrame(pc_projection, columns=['PC{}'.format(i+1) for i in range(n_pcs)])\n",
    "pca_df['structure'] = ensemble_df.index\n",
    "\n",
    "# print out the PCs\n",
    "display(pd.DataFrame(pca_df).head())\n",
    "\n",
    "# show table of variances explained by each PC\n",
    "display(pd.DataFrame((pc.cumulated_variance*100).round(), columns=['PC cumulated variance']).head())\n",
    "\n",
    "#drop the structure column for plotting\n",
    "pca_df_pairgrid = pca_df.drop('structure', axis=1)\n",
    "g = sns.PairGrid(pca_df_pairgrid)\n",
    "g.map(plt.scatter, marker='.')\n",
    "plt.show()\n",
    "pca_df_pairgrid = None\n",
    "\n",
    "# add principal components to the dataframe - making sure the structures are in the same order\n",
    "pca_df = pca_df.sort_values(by=['structure'])\n",
    "pca_df = pca_df.reset_index(drop=True)\n",
    "ensemble_df['PC1'] = pca_df['PC1'].values\n",
    "ensemble_df['PC2'] = pca_df['PC2'].values\n",
    "ensemble_df['PC3'] = pca_df['PC3'].values\n",
    "\n",
    "# visualisation\n",
    "n_pcs = 3\n",
    "for i in range(n_pcs):\n",
    "    pc_n =    pc.p_components[:, i]\n",
    "    trans_n =   pc_projection[:, i]\n",
    "    projected = np.outer(trans_n, pc_n) + pc.mean.flatten()\n",
    "    coordinates = projected.reshape(len(trans_n), -1, 3)\n",
    "    \n",
    "    proj_n = mda.Merge(u.select_atoms(rmsd_selection))\n",
    "    proj_n.load_new(coordinates)\n",
    "    \n",
    "    # write this to a multistate pdb file\n",
    "    with mda.Writer(base_directory + 'pca{}.pdb'.format(i+1), proj_n.atoms.n_atoms) as W:\n",
    "        for ts in proj_n.trajectory:\n",
    "            W.write(proj_n.atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select references structures for which to perform rmsd calculations below\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "# min samples is 10 percent of size of ensemble\n",
    "clustering_min_samples = int(len(ensemble_df) * 0.01)\n",
    "\n",
    "cluster = HDBSCAN(min_samples=clustering_min_samples, store_centers=\"medoid\").fit(ensemble_df[['PC1', 'PC2', 'PC3']])\n",
    "\n",
    "# add the cluster labels to the dataframe\n",
    "ensemble_df['cluster'] = cluster.labels_\n",
    "\n",
    "# get the number of clusters that are not noise (-1)\n",
    "num_clusters = len(set(cluster.labels_)) - (1 if -1 in cluster.labels_ else 0)\n",
    "\n",
    "# plot the clusters\n",
    "#plt.scatter(ensemble_df['PC1'], ensemble_df['PC2'], c=ensemble_df['cluster'], cmap='viridis')\n",
    "\n",
    "cluster_representatives = []\n",
    "for i in cluster.medoids_:\n",
    "    # lookup the relevant structures in the dataframe (using medoid guarantees this has been sampled)\n",
    "    cluster_representatives.append(ensemble_df[(ensemble_df['PC1'] == i[0]) & (ensemble_df['PC2'] == i[1]) & (ensemble_df['PC3'] == i[2])]['structure'].values[0])\n",
    "\n",
    "cluster_best_pLDDT = []\n",
    "# for each cluster number, get the structure with the highest pLDDT\n",
    "for i in range(num_clusters):\n",
    "    cluster_best_pLDDT.append(ensemble_df[ensemble_df['cluster'] == i].sort_values(by=['pDDLDT'], ascending=False)['structure'].values[0])\n",
    "    \n",
    "# plot the cluster representatives\n",
    "#for i in cluster_representatives:\n",
    "#    plt.scatter(ensemble_df[(ensemble_df['structure'] == i)]['PC1'], ensemble_df[(ensemble_df['structure'] == i)]['PC2'], c='red', marker='x', s=100)\n",
    "\n",
    "# display the cluster representatives in the dataframe\n",
    "#print('Cluster centroids (medoids):')\n",
    "#display(ensemble_df[ensemble_df['structure'].isin(ref_structures)])\n",
    "\n",
    "# display the best structures per cluster \n",
    "print('Top pLDDT per cluster:')\n",
    "display(ensemble_df[ensemble_df['structure'].isin(cluster_best_pLDDT)].sort_values(by=['pDDLDT'], ascending=False))\n",
    "\n",
    "ref_structures = cluster_representatives\n",
    "\n",
    "# diversity pick of structures from the over\n",
    "\n",
    "xaxis = 'PC1'\n",
    "yaxis = 'PC2'\n",
    "zaxis = 'PC3'\n",
    "\n",
    "# 3D plot of the ensemble\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(ensemble_df[xaxis], ensemble_df[yaxis], ensemble_df[zaxis], c=ensemble_df['cluster'], cmap='Paired', s=1, alpha=1)\n",
    "# plot the cluster representatives\n",
    "for i in cluster_representatives:\n",
    "    ax.scatter(ensemble_df[(ensemble_df['structure'] == i)][xaxis], ensemble_df[(ensemble_df['structure'] == i)][yaxis], ensemble_df[(ensemble_df['structure'] == i)][zaxis], c='black', marker='x', s=200)\n",
    "ax.set_xlabel(xaxis)\n",
    "ax.set_ylabel(yaxis)\n",
    "ax.set_zlabel(zaxis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSDs of all structures relative to the reference structures\n",
    "\n",
    "# define function to get rmsd to a structure\n",
    "def get_rmsd_to_structure(structure):\n",
    "    mobile = mda.Universe(structure_directory + structure, structure_directory + structure) # make universe\n",
    "    mobile.atoms.residues.resids += resid_offset                                            # renumber residues in mobile\n",
    "    rmsds = alignto(mobile, ref, select=rmsd_selection, match_atoms=True, weights=None)     # these ref selections are different becasue ref has different residue numbering\n",
    "    return [structure, rmsds[1]]                                                            # [1] = rmsd after alignment\n",
    "\n",
    "# define a function to calculate the SASA of a structure\n",
    "def get_sasa(structure):\n",
    "    p = PDBParser(QUIET=1)\n",
    "    struct = p.get_structure(structure, structure_directory + structure)\n",
    "    sr = ShrakeRupley(probe_radius=1.4, n_points=100)\n",
    "    sr.compute(struct, level=\"S\")\n",
    "    return [structure, struct.sasa]  \n",
    "\n",
    "\n",
    "if use_reference_structures:\n",
    "\n",
    "    # get average positions and make new universe with these positions\n",
    "    average_positions = (u_outward_open.atoms.positions + u_inward_open.atoms.positions) / 2\n",
    "    u_average = mda.Universe(outward_open_pdb, outward_open_pdb)\n",
    "    u_average.atoms.positions = average_positions\n",
    "\n",
    "    rmsd_to_outward = {}\n",
    "\n",
    "    # outward open\n",
    "    ref = u_outward_open\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        rmsds = list(tqdm(pool.imap(get_rmsd_to_structure, structures), total=len(structures)))\n",
    "    rmsd_to_outward = dict(rmsds)   # make this list of tuples into a dictionary\n",
    "\n",
    "    rmsd_to_inward = {}\n",
    "\n",
    "    # inward open\n",
    "    ref = u_inward_open\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        rmsds = list(tqdm(pool.imap(get_rmsd_to_structure, structures), total=len(structures)))\n",
    "    rmsd_to_inward = dict(rmsds)   # make this list of tuples into a dictionary\n",
    "\n",
    "    rmsd_to_average = {}\n",
    "\n",
    "    # average\n",
    "    ref = u_average\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        rmsds = list(tqdm(pool.imap(get_rmsd_to_structure, structures), total=len(structures)))\n",
    "    rmsd_to_average = dict(rmsds)   # make this list of tuples into a dictionary\n",
    "\n",
    "    sasa_dict = {}\n",
    "\n",
    "    # solvent accesible surface areas\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        sasas = list(tqdm(pool.imap(get_sasa, structures), total=len(structures)))\n",
    "    sasa_dict = dict(sasas)         # make this list of tuples into a dictionary\n",
    "\n",
    "    # add the rmsd values to the dataframe\n",
    "    ensemble_df['rmsd_to_outward'] = rmsd_to_outward.values()\n",
    "    ensemble_df['rmsd_to_inward'] = rmsd_to_inward.values()\n",
    "    ensemble_df['rmsd_to_average'] = rmsd_to_average.values()\n",
    "    # add the sasa values to the dataframe\n",
    "    ensemble_df['sasa'] = sasa_dict.values()\n",
    "\n",
    "else:\n",
    "\n",
    "    # get rmsd with respect ot each of hte reference structures\n",
    "    for ref_structure in ref_structures:\n",
    "        \n",
    "        # make a universe with the reference structure\n",
    "        ref = mda.Universe(structure_directory + ref_structure, structure_directory + ref_structure)\n",
    "\n",
    "        # get rmsd to reference structure\n",
    "        rmsds = {}\n",
    "        with Pool(processes=num_processes) as pool:\n",
    "            rmsds = list(tqdm(pool.imap(get_rmsd_to_structure, structures), total=len(structures)))\n",
    "        rmsds = dict(rmsds)\n",
    "\n",
    "        # add to the dataframe\n",
    "        ensemble_df['rmsd_to_' + ref_structure] = rmsds.values()\n",
    "\n",
    "    sasa_dict = {}\n",
    "\n",
    "    # solvent accesible surface areas\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        sasas = list(tqdm(pool.imap(get_sasa, structures), total=len(structures)))\n",
    "    sasa_dict = dict(sasas)         # make this list of tuples into a dictionary\n",
    "\n",
    "    # add the sasa values to the dataframe\n",
    "    ensemble_df['sasa'] = sasa_dict.values()\n",
    "\n",
    "display(ensemble_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure certain distances and add them to the dataframe\n",
    "cyto_helix_bundle_1  = '(' + 'resid 286-291 or resid 349-356 or resid 299-304' + ') and name CA' # '(' + 'resid 276-305 or resid 342-357' + ') and name CA'\n",
    "cyto_helix_bundle_2  = '(' + 'resid 147-152 or resid 220-225' + ') and name CA' # '(' + 'resid 139-166 or resid 213-241' + ') and name CA'\n",
    "lumen_helix_bundle_1 = '(' + 'resid 178-182 or resid 203-207' + ') and name CA'\n",
    "lumen_helix_bundle_2 = '(' + 'resid 121-125 or resid 319-323' + ') and name CA'\n",
    "\n",
    "# calculate certain distances e.g. to use as a CV\n",
    "def calc_distance(structure, selection1, selection2):\n",
    "        universe = mda.Universe(structure_directory + structure, structure_directory + structure) # make universe\n",
    "        universe.atoms.residues.resids += resid_offset      \n",
    "        sel1_coords = universe.select_atoms(selection1).positions\n",
    "        sel2_coords = universe.select_atoms(selection2).positions\n",
    "        sel1_coords_avg = np.mean(sel1_coords, axis=0)  # geometric average = COM when all atoms are Ca\n",
    "        sel2_coords_avg = np.mean(sel2_coords, axis=0)  # geometric average = COM when all atoms are Ca\n",
    "        distance = np.linalg.norm(sel1_coords_avg - sel2_coords_avg)\n",
    "        return distance\n",
    "\n",
    "# calculate distances\n",
    "distances = {}\n",
    "for structure in structures:\n",
    "    distances[structure] = calc_distance(structure, cyto_helix_bundle_1, cyto_helix_bundle_2)\n",
    "\n",
    "# add the distances to the dataframe\n",
    "ensemble_df['cyto_helix_bundle_separation'] = distances.values()\n",
    "\n",
    "distances = {}\n",
    "for structure in structures:\n",
    "    distances[structure] = calc_distance(structure, lumen_helix_bundle_1, lumen_helix_bundle_2)\n",
    "\n",
    "# add the distances to the dataframe\n",
    "ensemble_df['lumen_helix_bundle_separation'] = distances.values()\n",
    "   \n",
    "#  show\n",
    "display(ensemble_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plots\n",
    "if use_reference_structures:\n",
    "\n",
    "    # display tables of closest to reference structures, lowest sasa, and most different\n",
    "    print('Closest to outward open:')\n",
    "    display(ensemble_df.sort_values(by=['rmsd_to_outward']).head(1)[['structure', 'pDDLDT', 'rmsd_to_outward', 'PC1', 'PC2']])\n",
    "    print('Closest to inward open:')\n",
    "    display(ensemble_df.sort_values(by=['rmsd_to_inward']).head(1)[['structure', 'pDDLDT', 'rmsd_to_inward', 'PC1', 'PC2']])\n",
    "\n",
    "    # plot the rmsd to the reference structures\n",
    "    plt.scatter(ensemble_df['rmsd_to_outward'], ensemble_df['rmsd_to_inward'], c=ensemble_df['pDDLDT'], cmap='coolwarm')\n",
    "    plt.xlabel('RMSD to outward open')\n",
    "    plt.ylabel('RMSD to inward open')\n",
    "    plt.colorbar(label='pLDDT')\n",
    "    #limits\n",
    "    plt.xlim(0)\n",
    "    plt.ylim(0)\n",
    "    plt.show()\n",
    "\n",
    "        # plot the rmsd to the reference structures\n",
    "    plt.scatter(ensemble_df['rmsd_to_outward'], ensemble_df['rmsd_to_inward'], c=ensemble_df['sasa'], cmap='coolwarm')\n",
    "    plt.xlabel('RMSD to outward open')\n",
    "    plt.ylabel('RMSD to inward open')\n",
    "    plt.colorbar(label='SASA')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # plot histogram of rmsd to outward open\n",
    "    plt.hist(ensemble_df['rmsd_to_outward'], bins=20, alpha=0.5, label='outward open')\n",
    "    plt.hist(ensemble_df['rmsd_to_inward'], bins=20, alpha=0.5, label='inward open')\n",
    "\n",
    "else:\n",
    "\n",
    "    # display tables of closest to reference structures, lowest sasa, and most different\n",
    "    print('Represnetative structures from each cluster:')\n",
    "    for i in ref_structures:\n",
    "        display(ensemble_df.sort_values(by=['rmsd_to_' + i]).head(1)[['structure', 'pDDLDT', 'PC1', 'PC2']])\n",
    "    print('Lowest 3 SASA:')\n",
    "    display(ensemble_df.sort_values(by=['sasa']).head(3)[['structure', 'cluster', 'pDDLDT', 'PC1', 'PC2']])\n",
    "    \n",
    "    \n",
    "    # plot ensemble\n",
    "    plt.scatter(ensemble_df[plot_variable_1], ensemble_df[plot_variable_2], c=ensemble_df[plot_variable_3], cmap='coolwarm')\n",
    "    plt.xlabel(plot_variable_1)\n",
    "    plt.ylabel(plot_variable_2)\n",
    "    plt.colorbar(label=plot_variable_3)\n",
    "    \n",
    "    # annotate reference structures\n",
    "    for i in ref_structures:\n",
    "        plt.scatter(ensemble_df.loc[ensemble_df['structure'] == i][plot_variable_1], ensemble_df.loc[ensemble_df['structure'] == i][plot_variable_2], c='black', marker='x', s=100)\n",
    "        plt.annotate(i, (ensemble_df.loc[ensemble_df['structure'] == i][plot_variable_1], ensemble_df.loc[ensemble_df['structure'] == i][plot_variable_2]))\n",
    "    plt.show()\n",
    "    \n",
    "    # for each reference structure, plot the histogram of rmsd to that structure\n",
    "    for i in ref_structures:\n",
    "        plt.hist(ensemble_df['rmsd_to_' + i], bins=20, alpha=0.5, label=i)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('RMSD')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlim(0)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10, 2)\n",
    "    plt.show()\n",
    "    \n",
    "    # plot 2 pane histogram of SASA and pLDDT scores\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.hist(ensemble_df['sasa'], bins=20, alpha=0.5, label='sasa', color='C3')\n",
    "    ax1.set_xlabel('SASA')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax2.hist(ensemble_df['pDDLDT'], bins=20, alpha=0.5, label='pDDLDT')\n",
    "    ax2.set_xlabel('pDDLDT')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10, 2)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel calculation of the pairwise RMSD matrix (for the MC energy function used in the next cell)\n",
    "\n",
    "## load backup matrix\n",
    "rmsd_matrix = np.load(base_directory + 'rmsd_matrix_backup_90_bigsample.npy') # 'rmsd_matrix_80_6_0_3326-strucs.npy'\n",
    "\n",
    "# # Initialize a matrix to store pairwise RMSD values\n",
    "# rmsd_matrix = np.zeros((len(ensemble_df), len(ensemble_df)))\n",
    "# \n",
    "# # function to calculate RMSD values in parallel\n",
    "# def calculate_rmsd(i):\n",
    "# \n",
    "#     rmsd_values = []\n",
    "# \n",
    "#     # setup universe for i-th structure\n",
    "#     mobile = mda.Universe(structure_directory + ensemble_df['structure'].values[i], structure_directory + ensemble_df['structure'].values[i])\n",
    "#     if mobile.atoms.residues.resids[0] == 1: # if first resID == one, then residues are already numbered correctly\n",
    "#         mobile.atoms.residues.resids += resid_offset\n",
    "# \n",
    "#     for j in range(len(ensemble_df)):\n",
    "# \n",
    "#         # setup universe for j-th structure\n",
    "#         ref = mda.Universe(structure_directory + ensemble_df['structure'].values[j], structure_directory + ensemble_df['structure'].values[j])\n",
    "#         if ref.atoms.residues.resids[0] == 1: # if first resID == one, then residues are already numbered correctly\n",
    "#             ref.atoms.residues.resids += resid_offset\n",
    "# \n",
    "#         # Calculate RMSD and store it in the list\n",
    "#         rmsd = alignto(mobile, ref, select=rmsd_selection, match_atoms=True)[1]\n",
    "#         rmsd_values.append(rmsd)\n",
    "# \n",
    "#     mobile = None       # Release memory by setting the loaded structures to None\n",
    "#     ref = None\n",
    "# \n",
    "#     return rmsd_values\n",
    "# \n",
    "# # Perform parallel computation of RMSD values\n",
    "# with Pool(processes=num_processes) as pool:\n",
    "#     results = list(tqdm(pool.imap(calculate_rmsd, range(len(ensemble_df))), total=len(ensemble_df)))\n",
    "# \n",
    "# # Update the rmsd_matrix with the results\n",
    "# for i, rmsd_values in enumerate(results):\n",
    "#     rmsd_matrix[i] = rmsd_values\n",
    "# \n",
    "# results = None  # Release memory by setting the results to None\n",
    "# \n",
    "# np.save(base_directory + 'rmsd_matrix.npy', rmsd_matrix)    # save the matrix (to avoid recomputing it in the future)\n",
    "# \n",
    "# # Display progress information\n",
    "# print('Calculation completed for', len(ensemble_df), 'pairs of structures', end='\\r')\n",
    "# \n",
    "# # plot the matrix\n",
    "# plt.imshow(rmsd_matrix, cmap='inferno')\n",
    "# plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a matrix of pairwise SASA differences\n",
    "sasa_matrix = np.zeros((len(ensemble_df), len(ensemble_df)))\n",
    "n_elements = len(ensemble_df) * len(ensemble_df)\n",
    "\n",
    "# all pairwise sasas (these can just be subtracted from eachother)\n",
    "for i in range(len(ensemble_df)):\n",
    "    for j in range(len(ensemble_df)):\n",
    "        sasa_matrix[i,j] = (ensemble_df['sasa'].iloc[i] - ensemble_df['sasa'].iloc[j])\n",
    "        #sasa_matrix[i,j] = (ensemble_df['sasa'][i] - ensemble_df['sasa'][j])\n",
    "\n",
    "# show the matrix\n",
    "plt.imshow(sasa_matrix, cmap='coolwarm')\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a matrix of differences in collective variable between structures\n",
    "cv_matrix = np.zeros((len(ensemble_df), len(ensemble_df)))\n",
    "n_elements = len(ensemble_df) * len(ensemble_df)\n",
    "\n",
    "# all pairwise difference in collective variable (these can just be subtracted from eachother)\n",
    "for i in range(len(ensemble_df)):\n",
    "    for j in range(len(ensemble_df)):\n",
    "        cv_matrix[i,j] = (ensemble_df[collective_variable].iloc[i] - ensemble_df[collective_variable].iloc[j])\n",
    "        #cv_matrix[i,j] = (ensemble_df[collective_variable][i] - ensemble_df[collective_variable][j])\n",
    "\n",
    "# show the matrix\n",
    "plt.imshow(cv_matrix, cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign bins for binned mc path finding\n",
    "\n",
    "use_defined_end_states = True\n",
    "\n",
    "occluded_state = 'msa-16-32_rank_024.pdb'\n",
    "#lumen_open 'msa-128-256_rank_1163.pdb'\n",
    "#cyto open 'msa-32-64_rank_127.pdb'\n",
    "\n",
    "# end states to interpolate between values of the CV of\n",
    "end_state_1 = 'msa-128-256_rank_1163.pdb' #'msa-32-64_rank_127.pdb'\n",
    "end_state_2 = occluded_state\n",
    "\n",
    "# get the values of the CV for these structures\n",
    "end_state_1_cv_value = ensemble_df[ensemble_df['structure'] == end_state_1][collective_variable].values[0]\n",
    "end_state_2_cv_value = ensemble_df[ensemble_df['structure'] == end_state_2][collective_variable].values[0]\n",
    "\n",
    "if use_defined_end_states == True:\n",
    "    max_value = end_state_1_cv_value\n",
    "    min_value = end_state_2_cv_value\n",
    "else:\n",
    "    # just get the min and max value of the end states globally, like before\n",
    "    max_value = ensemble_df[collective_variable].max()\n",
    "    min_value = ensemble_df[collective_variable].min()\n",
    "\n",
    "\n",
    "## get the ideal collective variable values per window between min and max_value using linear interpolation\n",
    "ideal_window_values = np.zeros(mc_n_bins)\n",
    "for i in range(mc_n_bins):\n",
    "    ideal_window_values[i] = np.interp(i, [0, mc_n_bins-1], [max_value, min_value])\n",
    "ideal_initial_path = []\n",
    "for i in ideal_window_values:\n",
    "    ideal_initial_path.append(ensemble_df[collective_variable].sub(i).abs().idxmin())\n",
    "\n",
    "# replace the first and last structures in the ideal path with the end states\n",
    "if use_defined_end_states == True:\n",
    "    ideal_initial_path[0] = ensemble_df[ensemble_df['structure'] == end_state_1].index[0]\n",
    "    ideal_initial_path[-1] = ensemble_df[ensemble_df['structure'] == end_state_2].index[0]\n",
    "\n",
    "## set up the bins for the binned path\n",
    "ensemble_df['closest_ideal_structure'] = ensemble_df[collective_variable].apply(lambda x: ideal_initial_path[np.argmin(np.abs(ideal_window_values - x))])\n",
    "ensemble_df['bin'] = ensemble_df['closest_ideal_structure'].apply(lambda x: ideal_initial_path.index(x))\n",
    "\n",
    "\n",
    "# plotting\n",
    "#display(ensemble_df)\n",
    "plt.hist(ensemble_df['bin'], bins=mc_n_bins)\n",
    "plt.show()\n",
    "\n",
    "# show bin distribution\n",
    "plt.scatter(ensemble_df[plot_variable_1], ensemble_df[plot_variable_2], c=ensemble_df['bin'], cmap='tab20c')\n",
    "plt.colorbar()\n",
    "\n",
    "# plot the initial guesses\n",
    "for i in range(mc_n_bins):\n",
    "    plt.scatter(ensemble_df.loc[ensemble_df.index == ideal_initial_path[i]][plot_variable_1], ensemble_df.loc[ensemble_df.index == ideal_initial_path[i]][plot_variable_2], marker='x', color='black', s=10)\n",
    "# label the ideal structures wit htheir associated bin\n",
    "for i in range(mc_n_bins):\n",
    "    plt.annotate(i, (ensemble_df.loc[ensemble_df.index == ideal_initial_path[i]][plot_variable_1], ensemble_df.loc[ensemble_df.index == ideal_initial_path[i]][plot_variable_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC path finding by exchanging structures within bins\n",
    "\n",
    "def calc_energy(path, path_length):\n",
    "\n",
    "    wf_rmsd = 1\n",
    "    wf_cv = 1\n",
    "\n",
    "    # for minimising rmsd between structures on path\n",
    "    energy_rmsd = 0\n",
    "    for i in range(path_length - 1):\n",
    "        energy_rmsd += (rmsd_matrix[np.where(ensemble_df.index.values == path[i])[0][0], np.where(ensemble_df.index.values == path[i+1])[0][0]]**2)\n",
    "    energy_rmsd = wf_rmsd * np.sqrt ( 1/path_length * energy_rmsd )\n",
    "\n",
    "    # for minimising pairwise differences in collective variable between structures on path\n",
    "    energy_cv = 0\n",
    "    for i in range(path_length -1):\n",
    "        energy_cv += (cv_matrix[np.where(ensemble_df.index.values == path[i])[0][0], np.where(ensemble_df.index.values == path[i+1])[0][0]]**2)\n",
    "    energy_cv = wf_cv * np.sqrt ( 1/path_length * energy_cv )\n",
    "\n",
    "    # final term is sum\n",
    "    energy = energy_rmsd + energy_cv\n",
    "\n",
    "    # final energy\n",
    "    return energy\n",
    "\n",
    "# define a function that runs monte carlo simulated annealing to optimise path smoothness\n",
    "def mc_path_optimisation(seed):\n",
    "\n",
    "    fixed_endpoints = True\n",
    "    mc_n_steps = 1000\n",
    "    initial_temperature = 0.00001\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # list of temperatures mapped to mc_n_steps increasing stepwise in 10 increments\n",
    "    final_temperature = initial_temperature*1000\n",
    "    temperatures = np.logspace(np.log10(initial_temperature), np.log10(final_temperature), num=mc_n_steps)\n",
    "\n",
    "    mc_path_length = mc_n_bins\n",
    "\n",
    "    mcpath = []\n",
    "\n",
    "    initial_guess_indices = []\n",
    "    initial_guess_indices = ideal_initial_path\n",
    "\n",
    "    mcpath = initial_guess_indices\n",
    "\n",
    "    # dictionary of paths and energies\n",
    "    path_energies = {}\n",
    "\n",
    "    # run n_mc_runs steps\n",
    "    for i in range(mc_n_steps):\n",
    "\n",
    "        temperature = temperatures[i]\n",
    "\n",
    "        # calculate energy of current path\n",
    "        energy = calc_energy(mcpath, mc_path_length)\n",
    "        \n",
    "        # propose an exchange of a random (not endstate) structure in the path with a random structure from the pool\n",
    "        new_mcpath = mcpath.copy()\n",
    "\n",
    "        if fixed_endpoints == True:\n",
    "            start_point = 1\n",
    "            end_point = len(new_mcpath)-1\n",
    "        else:\n",
    "            start_point = 0\n",
    "            end_point = len(new_mcpath)\n",
    "\n",
    "        # loop throguh new_mcpath\n",
    "        for point in range(start_point, end_point):\n",
    "            structure_in_path = mcpath[point]\n",
    "\n",
    "            # select a random structure in the same bin as the structure in the path\n",
    "            random_structure_in_pool = np.random.choice(ensemble_df[ensemble_df['bin'] == ensemble_df.loc[structure_in_path]['bin']].index.values)\n",
    "\n",
    "            # replace the random structure in the new path with the random structure from the pool\n",
    "            new_mcpath[np.where(new_mcpath == structure_in_path)[0][0]] = random_structure_in_pool\n",
    "\n",
    "            # calcualte energy of proposed path\n",
    "            new_energy = calc_energy(new_mcpath, mc_path_length)\n",
    "\n",
    "            # calculate the difference between the two energies\n",
    "            delta_energy = 0\n",
    "            delta_energy = new_energy - energy\n",
    "\n",
    "            # metropolis criterion\n",
    "\n",
    "            # if lower, accept the new path\n",
    "            if delta_energy < 0:\n",
    "\n",
    "                mcpath = new_mcpath\n",
    "                path_energies[new_energy] = mcpath\n",
    "\n",
    "            else:\n",
    "\n",
    "                # if higher, accept with probability e^(-difference/temp)\n",
    "                if np.random.rand() < np.exp(-delta_energy / temperature):\n",
    "\n",
    "                    mcpath = new_mcpath\n",
    "                    path_energies[new_energy] = mcpath\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "    # get the path with the lowest energy\n",
    "    final_energy = min(path_energies.keys())\n",
    "    final_path = path_energies[final_energy]\n",
    "    relaxation_energies = list(path_energies.keys())\n",
    "    final_path_structures = tuple(ensemble_df.loc[final_path]['structure'].values)\n",
    "\n",
    "    return [final_energy, final_path, final_path_structures, relaxation_energies]\n",
    "\n",
    "mc_n_runs = 100\n",
    "\n",
    "# run mc_n_bins runs in parallel\n",
    "with Pool(processes=num_processes) as pool:\n",
    "    mc_runs = list(tqdm(pool.imap(mc_path_optimisation, range(mc_n_runs)), total=mc_n_runs))\n",
    "\n",
    "# add the final energies and final path structures to a dataframe\n",
    "mc_runs_df = pd.DataFrame(mc_runs, columns=['energy', 'path', 'path structures', 'relaxation energies'])\n",
    "mc_runs_df = mc_runs_df.sort_values(by=['energy']) \n",
    "\n",
    "# display the best run\n",
    "display(mc_runs_df[['energy', 'path structures']].head(3))\n",
    "\n",
    "# plot relaxation of the best run\n",
    "for i in range(1, 4):\n",
    "    plt.plot(mc_runs_df.iloc[i]['relaxation energies'])\n",
    "\n",
    "# label\n",
    "plt.xlabel('MC step')\n",
    "plt.ylabel('Energy')\n",
    "plt.show()\n",
    "\n",
    "# calculate the energy for each point in the best path (mc_runs[1]) (for plotting)\n",
    "energy_over_path = []\n",
    "energy = {}\n",
    "# get hte \"path\" of the best run (in indices)\n",
    "test = mc_runs_df.iloc[1]['path']\n",
    "for i in range(mc_n_bins - 1):\n",
    "        energy[i] = (rmsd_matrix[np.where(ensemble_df.index.values == test[i])[0][0], np.where(ensemble_df.index.values == test[i+1])[0][0]]**2)\n",
    "        energy[i] = np.sqrt ( 1 * energy[i] )\n",
    "plt.plot(energy.values(), label='best')\n",
    "\n",
    "test = mc_runs_df.iloc[-1]['path']\n",
    "for i in range(mc_n_bins - 1):\n",
    "        energy[i] = (rmsd_matrix[np.where(ensemble_df.index.values == test[i])[0][0], np.where(ensemble_df.index.values == test[i+1])[0][0]]**2)\n",
    "        energy[i] = np.sqrt ( 1 * energy[i] )\n",
    "plt.plot(energy.values(), label='worst')\n",
    "# plot energy\n",
    "\n",
    "# plot energy\n",
    "plt.legend()\n",
    "# label axes\n",
    "plt.xlabel('point in path')\n",
    "plt.ylabel('MC energy')\n",
    "plt.show()\n",
    "\n",
    "# write out the best run to a multistate pdb file and append timestamp to filename\n",
    "with mda.Writer(base_directory + 'best_run.pdb', u.atoms.n_atoms) as W:\n",
    "    # get the structures of the lowest energy path\n",
    "    for structure in mc_runs_df.head(1)['path structures'].values[0]:\n",
    "        # if structure is the reference, dont renumber residues\n",
    "        if structure == 'ref_outward.pdb' or structure == 'ref_inward.pdb':\n",
    "            # use a non-reference structure to make the universe\n",
    "            u = mda.Universe(structure_directory + structure, \n",
    "                             structure_directory + structure) # make universe\n",
    "            u.atoms.residues.resids += resid_offset \n",
    "            u.atoms.segments.segids = 'A'\n",
    "            u.atoms.chainIDs = 'A'\n",
    "            W.write(u.select_atoms('protein'))\n",
    "        else:\n",
    "            u = mda.Universe(structure_directory + structure, \n",
    "                             structure_directory + structure) # make universe\n",
    "            u.atoms.residues.resids += resid_offset\n",
    "            W.write(u.select_atoms('protein'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the best path\n",
    "\n",
    "# plot all the structures\n",
    "plt.scatter(ensemble_df[plot_variable_1], ensemble_df[plot_variable_2], c=ensemble_df['bin'], cmap='tab20c', alpha=0.8)\n",
    "\n",
    "# for each structure in best run, plot the rmsd to outward and inward from the ensemble_df dataframe\n",
    "for structure in mc_runs_df.head(1)['path'].values[0]:\n",
    "    plt.scatter(ensemble_df.loc[structure][plot_variable_1], ensemble_df.loc[structure][plot_variable_2], c='black')\n",
    "    #plt.annotate(structure, (ensemble_df.loc[structure][plot_variable_1], ensemble_df.loc[structure][plot_variable_2]))   \n",
    "    #plt.annotate(ensemble_df.loc[structure]['structure'], (ensemble_df.loc[structure][plot_variable_1], ensemble_df.loc[structure][plot_variable_2]), xytext=(ensemble_df.loc[structure][plot_variable_1], ensemble_df.loc[structure][plot_variable_2]+0.5), ha='center', va='bottom', arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.2'))\n",
    "\n",
    "    # annotate them with their bin number\n",
    "    plt.annotate(ensemble_df.loc[structure]['bin'], (ensemble_df.loc[structure][plot_variable_1], ensemble_df.loc[structure][plot_variable_2]), xytext=(ensemble_df.loc[structure][plot_variable_1], ensemble_df.loc[structure][plot_variable_2]+0.5), ha='center', va='bottom', arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.2'))\n",
    "\n",
    "# draw connecting lines\n",
    "for i in range(len(mc_runs_df.head(1)['path'].values[0]) - 1):\n",
    "    # get the rmsd to outward and inward from ensemble_df\n",
    "    x1 = ensemble_df.loc[mc_runs_df.head(1)['path'].values[0][i]][plot_variable_1]\n",
    "    y1 = ensemble_df.loc[mc_runs_df.head(1)['path'].values[0][i]][plot_variable_2]\n",
    "    x2 = ensemble_df.loc[mc_runs_df.head(1)['path'].values[0][i+1]][plot_variable_1]\n",
    "    y2 = ensemble_df.loc[mc_runs_df.head(1)['path'].values[0][i+1]][plot_variable_2]\n",
    "    plt.plot([x1, x2], [y1, y2], c='black', alpha=0.6, linestyle='dashed')\n",
    "\n",
    "# plot the initial guesses\n",
    "for i in range(mc_n_bins):\n",
    "    plt.scatter(ensemble_df.loc[ensemble_df.index == ideal_initial_path[i]][plot_variable_1], ensemble_df.loc[ensemble_df.index == ideal_initial_path[i]][plot_variable_2], marker='x', color='black', s=10)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(plot_variable_1)\n",
    "plt.ylabel(plot_variable_2)\n",
    "plt.show()\n",
    "\n",
    "# # plot SASA\n",
    "# for structure in mc_runs_df.head(1)['path'].values[0]:\n",
    "#     # plot rmsd to each end state on x and y axis and colour by sasa\n",
    "#     plt.scatter(ensemble_df.loc[structure]['rmsd_to_outward'], ensemble_df.loc[structure]['rmsd_to_inward'], c=ensemble_df.loc[structure]['sasa'], cmap='coolwarm')\n",
    "# #labels\n",
    "# plt.xlabel('rmsd_to_outward')\n",
    "# plt.ylabel('rmsd_to_inward')\n",
    "# #plt.ylim(ensemble_df['rmsd_ratio_outwardness'].min(), ensemble_df['rmsd_ratio_outwardness'].max())\n",
    "# fig = plt.gcf()\n",
    "# #fig.set_size_inches(15, 2)\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()\n",
    "\n",
    "#print('Best Run:')\n",
    "#mc_runs_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how good this is\n",
    "\n",
    "# plot the ideal window values\n",
    "plt.plot(ideal_window_values, label='ideal', color='black', alpha=0.2)\n",
    "# show dots\n",
    "plt.scatter(range(mc_n_bins), ideal_window_values, c=range(mc_n_bins), cmap='coolwarm', alpha=0.5)\n",
    "\n",
    "# get the actual values\n",
    "actual_window_values = np.zeros(mc_n_bins)\n",
    "for i in range(mc_n_bins):\n",
    "    actual_window_values[i] = ensemble_df.loc[mc_runs_df.head(1)['path'].values[0][i]][collective_variable]\n",
    "\n",
    "# print the ideal and actual values in a table\n",
    "print('Ideal vs Actual Values:')\n",
    "display(pd.DataFrame({'ideal': ideal_window_values, 'actual': actual_window_values}))\n",
    "\n",
    "# plot the actual values\n",
    "plt.plot(actual_window_values, label='actual', c='black', alpha=0.8)\n",
    "# show dots\n",
    "plt.scatter(range(mc_n_bins), actual_window_values, c=range(mc_n_bins), cmap='coolwarm')\n",
    "\n",
    "# plot vertical lines between the two\n",
    "for i in range(mc_n_bins):\n",
    "    plt.plot([i, i], [ideal_window_values[i], actual_window_values[i]], c='black', alpha=0.2, linestyle='dashed')\n",
    "\n",
    "if ideal_initial_path == True:\n",
    "\n",
    "    # alternate where initial guessi ncides are just those of the structures closest to the ideal values of the collective variable (from a linear interpolation)\n",
    "    initial_guess_indices = []\n",
    "    # get the ideal values of the collective variable\n",
    "\n",
    "    # enpoint values are min adn max of the collective variable\n",
    "    ideal_values = np.linspace(ensemble_df[collective_variable].min(), ensemble_df[collective_variable].max(), mc_n_bins)\n",
    "\n",
    "    # get the closest structure to each ideal value\n",
    "    for i in ideal_values:\n",
    "        initial_guess_indices.append(ensemble_df[collective_variable].sub(i).abs().idxmin())\n",
    "    # plot the initial guesses\n",
    "    for i in range(mc_n_bins):\n",
    "        plt.scatter(i, ensemble_df.loc[initial_guess_indices[i]][collective_variable], marker='x', color='black')\n",
    "    print(len(initial_guess_indices))   \n",
    "\n",
    "# x axis labels\n",
    "plt.xticks(range(mc_n_bins), range(mc_n_bins))\n",
    "plt.xlabel('Window Number')\n",
    "plt.ylabel(collective_variable + 'CV')\n",
    "plt.legend()\n",
    "\n",
    "# add a title\n",
    "plt.title('Ideal vs Actual Window CV Values')\n",
    "plt.show()\n",
    "\n",
    "# now just plot the offsets (absolute) as a bar chart\n",
    "offsets = np.abs(ideal_window_values - actual_window_values)\n",
    "plt.bar(range(mc_n_bins), offsets, color='black', alpha=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up umbrella sampling windows\n",
    "from MDAnalysis.analysis import align, rms\n",
    "\n",
    "def align_universe(mobile, ref):\n",
    "# aligns mobile universe to first frame of reference universe\n",
    "    alignment_selection = 'protein and name CA'\n",
    "    mobile.trajectory[-1]  # set mobile trajectory to last frame\n",
    "    ref.trajectory[0]  # set reference trajectory to first frame\n",
    "    mobile_ca = mobile.select_atoms(alignment_selection)\n",
    "    ref_ca = ref.select_atoms(alignment_selection)\n",
    "    rms.rmsd(\n",
    "             mobile_ca.positions, \n",
    "             ref_ca.positions, \n",
    "             superposition=False\n",
    "             )\n",
    "    aligner = align.AlignTraj(mobile, \n",
    "                              ref, \n",
    "                              select=alignment_selection,\n",
    "                              in_memory=True).run()\n",
    "    return mobile\n",
    "\n",
    "#template universe embedded in lipid bilyer with gromacs topology etc\n",
    "template_universe = mda.Universe(base_directory + 'template.pdb', base_directory + 'template.pdb')\n",
    "\n",
    "umbrella_sampling_directory = base_directory + 'umbrella_sampling/'\n",
    "window_directories = []\n",
    "\n",
    "# make a directory for each window\n",
    "for i in range(mc_n_bins):\n",
    "    window_directories.append(umbrella_sampling_directory + 'window_' + str(i))\n",
    "    if not os.path.exists(window_directories[i]):\n",
    "        os.makedirs(window_directories[i])\n",
    "\n",
    "#gmx_exec = '/biggin/b212/bioc1781/software/gromacs/2022.4_CUDA_AVX2_plumed/bin/gmx'\n",
    "#\n",
    "#grompp = gmx.commandline_operation('gmx', 'grompp',\n",
    "#                                   input_files={\n",
    "#                                       '-f': mdpfile,\n",
    "#                                       '-p': solvate.output.file['-p'],\n",
    "#                                       '-c': solvate.output.file['-o'],\n",
    "#                                       '-po': mdout_mdp,\n",
    "#                                   },\n",
    "#                                   output_files={'-o': tprfile})\n",
    "\n",
    "\n",
    "# for each window, replace the coordinates of the protein in the template universe with the coordinates of the structure at the corresponding index in the best path\n",
    "for i in range(mc_n_bins):\n",
    "\n",
    "    # write the relevant structure to a pdb file in the window directory\n",
    "    # pass the structure to gmx pdb2gmx to get the correct atom order (discard topology)\n",
    "\n",
    "\n",
    "    ## make a copy of the template universe\n",
    "    #u = template_universe.copy()\n",
    "    #\n",
    "    ## make a universe out of the structure corresponding to the index in the best path\n",
    "    #structure = mc_runs_df.head(1)['path structures'].values[0][i]\n",
    "    #u_structure = mda.Universe(structure_directory + structure, structure_directory + structure)\n",
    "    ## select the protein atoms in the template universe\n",
    "    #protein = u.select_atoms('protein')\n",
    "    ## align the structure to the template universe\n",
    "    #u_structure = align_universe(u_structure, u)\n",
    "    ## replace the coordinates of the protein in the template universe with the coordinates of the protein in the structure universe\n",
    "    #protein.positions = u_structure.select_atoms('protein').positions\n",
    "#\n",
    "    ## write out the universe to a pdb file\n",
    "    #u.atoms.write(window_directories[i] + '/window_' + str(i) + '.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propka\n",
    "from propkatraj import PropkaTraj\n",
    "\n",
    "residues = ['205', '305']\n",
    "# copy dataframe \n",
    "pka_df = ensemble_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## write a multistate pdb file for the whole ensemble so MDA will interpret it as a trajectory\n",
    "#with mda.Writer(base_directory + 'ensemble.pdb', u.atoms.n_atoms) as W:\n",
    "#    for structure in ensemble_df['structure']:\n",
    "#        u = mda.Universe(structure_directory + structure, \n",
    "#                         structure_directory + structure)\n",
    "#        # renumber for consistency and to match selection token   s     \n",
    "#        u.atoms.residues.resids += resid_offset\n",
    "#        u.atoms.segments.segids = 'A'\n",
    "#        u.atoms.chainIDs = 'A'\n",
    "#        W.write(u.select_atoms(rmsd_selection))\n",
    " \n",
    " ## make a universe containing all the structures\n",
    " #u = mda.Universe(base_directory + \"ensemble.pdb\")\n",
    " #\n",
    " #u.atoms.residues.resids += resid_offset\n",
    " #u.atoms.segments.segids = 'A'\n",
    " #u.atoms.chainIDs = 'A'\n",
    " #\n",
    " #pkatraj = PropkaTraj(u, select='protein', skip_failure=True, Verbose=True)\n",
    " #pkatraj.run()   # creates a pandas dataframe with the pka values for each residue in each frame results.pkas\n",
    " #struc_pkas = pkatraj.results.pkas.describe()\n",
    "\n",
    "# align the ensemble to the selection token\n",
    "#aligner = mda.analysis.align.AlignTraj(u, u, select=rmsd_selection, in_memory=True).run()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# make a universe object containing all the structures\n",
    "#u = mda.Universe(structure_directory + structures[0], structure_directory + structures[0])\n",
    "\n",
    "# loop thorugh the structures making a universe object for each\n",
    "# make a black array to store results for all structures\n",
    "results = np.zeros((len(structures), len(residues)))\n",
    "print(structures[1:])\n",
    "for structure in structures[1:]:\n",
    "    u = mda.Universe(structure_directory + structure, structure_directory + structure)\n",
    "    # renumber residues\n",
    "    u.atoms.residues.resids += resid_offset\n",
    "    u.atoms.segments.segids = 'A'\n",
    "    u.atoms.chainIDs = 'A'\n",
    "\n",
    "    #print(structure)\n",
    "    pkatraj = PropkaTraj(u, select='protein', skip_failure=True, Verbose=True)\n",
    "    pkatraj.run()   # creates a pandas dataframe with the pka values for each residue in each frame results.pkas\n",
    "    #print(pkatraj.results.pkas)\n",
    "    struc_pkas = pkatraj.results.pkas.describe()\n",
    "    # add to results array\n",
    "    results[np.where(structures == structure)[0][0]] = struc_pkas.loc['mean', residues]\n",
    "\n",
    "\n",
    "  \n",
    "  #for residue in residues:\n",
    "  #    # get the mean pka for the residue of interest\n",
    "  #    pka = struc_pkas.loc['mean', residue]\n",
    "  #    # add this pka value to a column labelled residue in the pka_df with the structure name as the index \n",
    "  #    pka_df.loc[structure, 'pka_' + str(residue)] = pka\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
