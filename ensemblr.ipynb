{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and settings\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import MDAnalysis as mda\n",
    "import seaborn as sns\n",
    "import nglview as nv                            # for visualisation\n",
    "from MDAnalysis.analysis.align import alignto   # for aligning structures\n",
    "from MDAnalysis.analysis.pca import PCA         # for PCA\n",
    "from Bio.PDB import PDBParser\n",
    "from Bio.PDB.DSSP import DSSP                   # for secondary structure selection\n",
    "from Bio.PDB.SASA import ShrakeRupley           # for SASA calculation\n",
    "from IPython.display import display             # for data frame display\n",
    "from multiprocessing import Pool                # for multiprocessing\n",
    "from tqdm import tqdm                           # for progress bars\n",
    "\n",
    "# surpress warnings\n",
    "warnings.filterwarnings(action='ignore', module='matplotlib')\n",
    "warnings.filterwarnings(action='ignore', module='mdanalysis')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# pandas settings\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# base directories\n",
    "base_directory = \"/biggin/b212/bioc1781/Projects/CTNS/human/monomer/red-msa/with-dropout/\" #\"/biggin/b212/bioc1781/Projects/KDELR/red-msa/\"  #\"/biggin/b212/bioc1781/Projects/CTNS/human/monomer/red-msa/with-dropout/\"\n",
    "structure_directory = base_directory + \"ensemble/\" # directory with the AF2 ensemble, cannot be the same as base directory\n",
    "\n",
    "# settings\n",
    "selection_for_writeout  = \"protein and name CA\"  # useful if you want all the outputs to have a specific selection\n",
    "conserved_residues      = \"resid 162 or resid 281 or resid 142 or resid 143 or resid 346 or resid 345 or resid 305 or resid 280 or resid 138 or resid 211 or resid 208 or resid 273 or resid 173 or resid 335 or resid 205 or resid 332 or resid 176 \" #\"resid 205 or resid 305 or resid 346 or resid 169 or resid 308 or resid 339 or resid 260 or resid 158 or resid 338 or resid 177 or resid 288 or resid 222 or resid 139 or resid 141 or resid 298 or resid 182 or resid 280 or resid 335 or resid 142 or resid 138 or resid 170 or resid 208 or resid 173 or resid 134\"        #'resid 205 or resid 305 or resid 346 or resid 169 or resid 308 or resid 339 or resid 260 or resid 158 or resid 338 or resid 177 or resid 288 or resid 222 or resid 139 or resid 141 or resid 298 or resid 182 or resid 280 or resid 335 or resid 142 or resid 138 or resid 170 or resid 208 or resid 173 or resid 134'\n",
    "excluded_residues       = \"\"        # resid 134-149\n",
    "resid_offset    = 115 #0 #115            # first resID in the reference structures if the chain does not start from 1\n",
    "# structure filtering   \n",
    "thresh_rmsd     = 10 #6             # threshold for discarding structures based on RMSD\n",
    "thresh_pLDDT    = 90                # threshold for discarding structures based on pLDDT\n",
    "thresh_sasa_coeff = 100             # multiple of highest sasa of the reference structure above which structures are discarded\n",
    "#diffmat_thresh  = 0                # threshold for ignoring atoms in RMSD calculations based on how much they differ between the reference structures\n",
    "# analysis  \n",
    "num_processes   = 12                # CPU cores to use for multiprocessing\n",
    "n_pcs           = 3                 # number of principal components to keep in PCA\n",
    "# monte carlo   \n",
    "mc_temp         = 500               # monte carlo temperature\n",
    "mc_wf_sasa      = 0                 # monte carlo energy function weight factor for SASA (check if this should be -ve, depends on order of subtraction and matrix indexing)\n",
    "mc_n_runs       = 1                 # number of monte carlo runs\n",
    "mc_n_bins       = 24                # so mc_n_bins is the total number of bins (because of reference structures and zero indexing)\n",
    "collective_variable = 'cyto_helix_bundle_separation'         # collective_variable to bin for path finding\n",
    "start_from_endstates = False\n",
    "# for plotting\n",
    "plot_variable_1     = 'lumen_helix_bundle_separation'   \n",
    "plot_variable_2     = 'cyto_helix_bundle_separation'\n",
    "plot_variable_3     = 'sasa'\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_processes)\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# reference structures\n",
    "use_reference_structures = True\n",
    "\n",
    "if use_reference_structures:\n",
    "\n",
    "    outward_open_pdb = base_directory + \"8DKE_cytosol_noNTD_hydrogens.pdb\"\n",
    "    inward_open_pdb  = base_directory + \"8DKI_lumen_noNTD_hydrogens.pdb\"\n",
    "\n",
    "    # universes for reference structures\n",
    "    u_outward_open = mda.Universe(outward_open_pdb, outward_open_pdb)\n",
    "    u_inward_open  = mda.Universe(inward_open_pdb, inward_open_pdb)\n",
    "\n",
    "    # align reference structures to eachother\n",
    "    \n",
    "    alignto(u_inward_open, u_outward_open, select='all', weights=\"mass\")\n",
    "\n",
    "    # write reindexed pdbs of reference structures for inclusion in the ensemble\n",
    "    u_outward_open.atoms.residues.resids -= resid_offset\n",
    "    u_outward_open.atoms.write(structure_directory + \"ref_outward.pdb\")\n",
    "    u_outward_open.atoms.residues.resids += resid_offset\n",
    "    #\n",
    "    u_inward_open.atoms.residues.resids -= resid_offset\n",
    "    u_inward_open.atoms.write(structure_directory + \"ref_inward.pdb\")\n",
    "    u_inward_open.atoms.residues.resids += resid_offset\n",
    "\n",
    "else:\n",
    "    outward_open_pdb = None\n",
    "    inward_open_pdb  = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve structures\n",
    "\n",
    "# user specific - do whatever you need here to get a dictionary of structures and their associated pLDDT values\n",
    "\n",
    "# # ONLY NEED TO RUN ONCE \n",
    "# # copy structures from outputs to ensemble\n",
    "# msa_depths = ['32-64', '64-128', '128-256', '256-512', '512-1024']\n",
    "# \n",
    "# for msa in msa_depths:\n",
    "#     structures_msa = [f for f in os.listdir(base_directory + 'output_' + msa) if f.endswith('.pdb')]\n",
    "#     for structure in structures_msa:\n",
    "#         # copy all the pdb files with \"_relaxed_\" in the name to the structure directory\n",
    "#         if \"_relaxed_\" in structure:\n",
    "#             # get the rank of the strcuture (last thing before the file extension)\n",
    "#             rank = 'rank_' + structure.split('_')[structure.split('_').index('rank')+1]\n",
    "#             os.system('cp ' + base_directory + 'output_' + msa + '/' + structure + ' ' + structure_directory + '/msa-' + msa + '_' + rank + '.pdb')\n",
    "# \n",
    "# # get list of all files in structure directory directory with the pdb extension\n",
    "# structures = [f for f in os.listdir(structure_directory) if f.endswith('.pdb')]\n",
    "# structures.sort()\n",
    "\n",
    "# associate structures with pLDDT values\n",
    "structure_and_pLDDT = {}\n",
    "\n",
    "msa_depths = ['8-16', '16-32', '32-64', '64-128', '128-256', '256-512', '512-1024']\n",
    "for msa in msa_depths:\n",
    "    structures_msa = [f for f in os.listdir(structure_directory) if f.endswith('.pdb') and msa in f and str(msa) in f]\n",
    "    log_file = structure_directory + msa + \"_log.txt\"   # look up the corresponding log file\n",
    "    for structure in structures_msa:\n",
    "        rank = 'rank_' + structure.split('_')[structure.split('_').index('rank')+1] # get the rank of the structure\n",
    "        rank = rank.split('.')[0]           # remove the file extension\n",
    "        with open(log_file, \"r\") as file:\n",
    "            for line in file:               # match the rank with the pLDDT value\n",
    "                if rank in line:\n",
    "                    pLDDT = line.split()[3].replace('pLDDT=','')\n",
    "                    structure_and_pLDDT[structure] = float(pLDDT)\n",
    "                    break\n",
    "\n",
    "print('Structures:', len(structure_and_pLDDT))\n",
    "\n",
    "# if no reference structures, chose 2 hightest pLDDT structures as references\n",
    "if not use_reference_structures:\n",
    "    # get the 2 structures with the highest pLDDT values\n",
    "    ref_structures = sorted(structure_and_pLDDT, key=structure_and_pLDDT.get, reverse=True)[:2]\n",
    "    print('Reference structures:', ref_structures)\n",
    "    # declare the appropriate variable (the name of the structure file)\n",
    "    outward_open_pdb = structure_directory + ref_structures[0]\n",
    "    inward_open_pdb  = structure_directory + ref_structures[1]\n",
    "    # universes for reference structures\n",
    "    u_outward_open = mda.Universe(outward_open_pdb, outward_open_pdb)\n",
    "    u_inward_open  = mda.Universe(inward_open_pdb, inward_open_pdb)\n",
    "\n",
    "    # align reference structures to eachother\n",
    "    alignto(u_inward_open, u_outward_open, select='protein', weights=\"mass\")\n",
    "\n",
    "    # write reindexed pdbs of reference structures for inclusion in the ensemble\n",
    "    #u_outward_open.atoms.residues.resids -= resid_offset\n",
    "    #u_outward_open.atoms.write(structure_directory + \"ref_outward.pdb\")\n",
    "    #u_outward_open.atoms.residues.resids += resid_offset\n",
    "    #\n",
    "    #u_inward_open.atoms.residues.resids -= resid_offset\n",
    "    #u_inward_open.atoms.write(structure_directory + \"ref_inward.pdb\")\n",
    "    #u_inward_open.atoms.residues.resids += resid_offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic \"clever\" selection definer\n",
    "\n",
    "# temporary universe\n",
    "u = u_outward_open\n",
    "\n",
    "# identify regions of secondary structure\n",
    "p = PDBParser()\n",
    "structure = p.get_structure('reference', outward_open_pdb)\n",
    "model = structure[0]\n",
    "dssp = DSSP(model, outward_open_pdb, dssp='mkdssp')\n",
    "\n",
    "helices = []\n",
    "sheets  = []\n",
    "loops   = []\n",
    "\n",
    "# get secondary structure labels for resIDs\n",
    "for key in dssp.keys():\n",
    "    if dssp[key][2] == 'H' or dssp[key][2] == 'G' or dssp[key][2] == 'I':\n",
    "        helices.append(key[1][1])\n",
    "    elif dssp[key][2] == 'E':\n",
    "        sheets.append(key[1][1])\n",
    "    elif dssp[key][2] == 'T' or dssp[key][2] == 'S':\n",
    "        loops.append(key[1][1])\n",
    "    \n",
    "helices = sorted(list(set(helices)))\n",
    "sheets = sorted(list(set(sheets)))\n",
    "loops = sorted(list(set(loops)))\n",
    "\n",
    "helices_contiguous = []\n",
    "sheets_contiguous  = []\n",
    "loops_contiguous   = []\n",
    "\n",
    "# get contiguous regions of secondary structure\n",
    "for i in range(len(helices)):\n",
    "    if i == 0:\n",
    "        helices_contiguous.append([helices[i]])\n",
    "    elif helices[i] == helices[i-1] + 1:\n",
    "        helices_contiguous[-1].append(helices[i])\n",
    "    else:\n",
    "        helices_contiguous.append([helices[i]])\n",
    "\n",
    "for i in range(len(sheets)):\n",
    "    if i == 0:\n",
    "        sheets_contiguous.append([sheets[i]])\n",
    "    elif sheets[i] == sheets[i-1] + 1:\n",
    "        sheets_contiguous[-1].append(sheets[i])\n",
    "    else:\n",
    "        sheets_contiguous.append([sheets[i]])\n",
    "\n",
    "for i in range(len(loops)):\n",
    "    if i == 0:\n",
    "        loops_contiguous.append([loops[i]])\n",
    "    elif loops[i] == loops[i-1] + 1:\n",
    "        loops_contiguous[-1].append(loops[i])\n",
    "    else:\n",
    "        loops_contiguous.append([loops[i]])\n",
    "\n",
    "selection_helices = []\n",
    "selection_sheets  = []\n",
    "selection_loops   = []\n",
    "\n",
    "# make mdanalysis selections corresponding to these regions\n",
    "for i in range(len(helices_contiguous)):\n",
    "    selection_helices.append('(resid %s-%s)' % (helices_contiguous[i][0], helices_contiguous[i][-1]))\n",
    "for i in range(len(sheets_contiguous)):\n",
    "    selection_sheets.append('(resid %s-%s)' % (sheets_contiguous[i][0], sheets_contiguous[i][-1]))\n",
    "for i in range(len(loops_contiguous)):\n",
    "    selection_loops.append('(resid %s-%s)' % (loops_contiguous[i][0], loops_contiguous[i][-1]))\n",
    "\n",
    "# conserved residues\n",
    "selection_conserved_residues = '((' + conserved_residues + ') and (name CA or name CG or name CZ* or name NZ))' \n",
    "\n",
    "# format selections\n",
    "selection_helices = ','.join(selection_helices)\n",
    "selection_helices = selection_helices.replace(',', ' or ')\n",
    "selection_sheets = ','.join(selection_sheets)\n",
    "selection_sheets = selection_sheets.replace(',', ' or ')\n",
    "selection_loops = ','.join(selection_loops)\n",
    "selection_loops = selection_loops.replace(',', ' or ')\n",
    "\n",
    "endstates = {}\n",
    "\n",
    "resids_from_ca_dist_diffmat = []\n",
    "\n",
    "## get the resid of the residues that differ by more than the threshold in absolute terms\n",
    "#above_thresh = np.where(abs(ca_dist_difference_matrix) >= diffmat_thresh)\n",
    "#for i in range(len(above_thresh[0])):\n",
    "#    resids_from_ca_dist_diffmat.append(u.atoms[above_thresh[0][i]].resid)\n",
    "#resids_from_ca_dist_diffmat = list(set(resids_from_ca_dist_diffmat))    # get unique residues\n",
    "\n",
    "# make a selection token for these residues\n",
    "selection_from_ca_dist_diffmat = []\n",
    "for i in range(len(resids_from_ca_dist_diffmat)):\n",
    "    selection_from_ca_dist_diffmat.append('resid %s' % resids_from_ca_dist_diffmat[i])\n",
    "selection_from_ca_dist_diffmat = ','.join(selection_from_ca_dist_diffmat)\n",
    "selection_from_ca_dist_diffmat = selection_from_ca_dist_diffmat.replace(',', ' or ')\n",
    "\n",
    "# final rmsd_selection for analysis\n",
    "rmsd_selection = '( ( (' + selection_helices + ') and name CA ) or ( (' + selection_loops + ') and name CA ) )' # or' + selection_conserved_residues #+ ' and not (resid 116-120 or resid 356-367)'\n",
    "if conserved_residues != \"\":\n",
    "    rmsd_selection += ' or ( (' + selection_helices + ') and' + selection_conserved_residues + ')'\n",
    "if excluded_residues != \"\":\n",
    "    rmsd_selection += ' and not (' + excluded_residues + ')'\n",
    "\n",
    "print(rmsd_selection)\n",
    "\n",
    "# write a pdb of the selection\n",
    "# select the rmsd_selection atoms in the outward open structure\n",
    "check = u.select_atoms(rmsd_selection)\n",
    "# write out the selection\n",
    "check.write(base_directory + 'rmsd_selection.pdb')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make main pandas dataframe and filter data\n",
    "ensemble_df = pd.DataFrame(index=structure_and_pLDDT.keys())\n",
    "# rename the index column to structure\n",
    "ensemble_df.index.names = ['structure']\n",
    "\n",
    "pLDDT_scores = {}\n",
    "plddt_df = pd.DataFrame(list(structure_and_pLDDT.items()), columns=['structure', 'pDDLDT']) # Convert the dictionary to a DataFrame\n",
    "\n",
    "# add entries to dictionary for the reference structures with pLDDT scores of 100\n",
    "if use_reference_structures:\n",
    "    plddt_df = pd.concat([plddt_df, pd.DataFrame({'structure': 'ref_outward.pdb', 'pDDLDT': 100}, index=[0])], ignore_index=True)   # append is deprecated, use concat instead\n",
    "    plddt_df = pd.concat([plddt_df, pd.DataFrame({'structure': 'ref_inward.pdb', 'pDDLDT': 100}, index=[0])], ignore_index=True)\n",
    "\n",
    "# Merge the two DataFrames on the structure column\n",
    "ensemble_df = pd.merge(ensemble_df, plddt_df, on='structure')\n",
    "\n",
    "# filter by pLDDT\n",
    "ensemble_df = ensemble_df[ensemble_df['pDDLDT'] > thresh_pLDDT]      # discard < thresh_pLDDT\n",
    "structures = list(ensemble_df['structure'])                          # apply the filter to the list of structure names\n",
    "\n",
    "display(ensemble_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PCA on the ensemble\n",
    "\n",
    "# write a multistate pdb file for the whole ensemble so MDA will interpret it as a trajectory\n",
    "with mda.Writer(base_directory + 'ensemble.pdb', u.atoms.n_atoms) as W:\n",
    "    for structure in ensemble_df['structure']:\n",
    "        u = mda.Universe(structure_directory + structure, \n",
    "                         structure_directory + structure)\n",
    "        # renumber for consistency and to match selection token   s     \n",
    "        u.atoms.residues.resids += resid_offset\n",
    "        u.atoms.segments.segids = 'A'\n",
    "        u.atoms.chainIDs = 'A'\n",
    "        W.write(u.select_atoms(rmsd_selection))\n",
    " \n",
    "# make a universe containing all the structures\n",
    "u = mda.Universe(base_directory + \"ensemble.pdb\")\n",
    "\n",
    "# align the ensemble to the selection token\n",
    "aligner = mda.analysis.align.AlignTraj(u, u, select=rmsd_selection, in_memory=True).run()\n",
    "\n",
    "# perform principal component analysis\n",
    "pc = PCA(u, select=rmsd_selection, align=True, mean=None, n_components=None).run()\n",
    "\n",
    "# project coorindates onto the principal components\n",
    "pc_projection = pc.transform(u.select_atoms(rmsd_selection), n_components=n_pcs)\n",
    "\n",
    "# make a dataframe to store the principal components\n",
    "pca_df = pd.DataFrame(pc_projection, columns=['PC{}'.format(i+1) for i in range(n_pcs)])\n",
    "pca_df['structure'] = ensemble_df.index\n",
    "\n",
    "# print out the PCs\n",
    "display(pd.DataFrame(pca_df).head())\n",
    "\n",
    "# show table of variances explained by each PC\n",
    "display(pd.DataFrame((pc.cumulated_variance*100).round(), columns=['PC cumulated variance']).head())\n",
    "\n",
    "#drop the structure column for plotting\n",
    "pca_df_pairgrid = pca_df.drop('structure', axis=1)\n",
    "g = sns.PairGrid(pca_df_pairgrid)\n",
    "g.map(plt.scatter, marker='.')\n",
    "plt.show()\n",
    "pca_df_pairgrid = None\n",
    "\n",
    "# add principal components to the dataframe - making sure the structures are in the same order\n",
    "pca_df = pca_df.sort_values(by=['structure'])\n",
    "pca_df = pca_df.reset_index(drop=True)\n",
    "ensemble_df['PC1'] = pca_df['PC1'].values\n",
    "ensemble_df['PC2'] = pca_df['PC2'].values\n",
    "ensemble_df['PC3'] = pca_df['PC3'].values\n",
    "\n",
    "# visualisation\n",
    "n_pcs = 3\n",
    "for i in range(n_pcs):\n",
    "    pc_n =    pc.p_components[:, i]\n",
    "    trans_n =   pc_projection[:, i]\n",
    "    projected = np.outer(trans_n, pc_n) + pc.mean.flatten()\n",
    "    coordinates = projected.reshape(len(trans_n), -1, 3)\n",
    "    \n",
    "    proj_n = mda.Merge(u.select_atoms(rmsd_selection))\n",
    "    proj_n.load_new(coordinates)\n",
    "    \n",
    "    # write this to a multistate pdb file\n",
    "    with mda.Writer(base_directory + 'pca{}.pdb'.format(i+1), proj_n.atoms.n_atoms) as W:\n",
    "        for ts in proj_n.trajectory:\n",
    "            W.write(proj_n.atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select references structures for which to perform rmsd calculations below\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "# min samples is 10 percent of size of ensemble\n",
    "clustering_min_samples = int(len(ensemble_df) * 0.01)\n",
    "\n",
    "cluster = HDBSCAN(min_samples=clustering_min_samples, store_centers=\"medoid\").fit(ensemble_df[['PC1', 'PC2', 'PC3']])\n",
    "\n",
    "# add the cluster labels to the dataframe\n",
    "ensemble_df['cluster'] = cluster.labels_\n",
    "\n",
    "# get the number of clusters that are not noise (-1)\n",
    "num_clusters = len(set(cluster.labels_)) - (1 if -1 in cluster.labels_ else 0)\n",
    "\n",
    "# plot the clusters\n",
    "#plt.scatter(ensemble_df['PC1'], ensemble_df['PC2'], c=ensemble_df['cluster'], cmap='viridis')\n",
    "\n",
    "cluster_representatives = []\n",
    "for i in cluster.medoids_:\n",
    "    # lookup the relevant structures in the dataframe (using medoid guarantees this has been sampled)\n",
    "    cluster_representatives.append(ensemble_df[(ensemble_df['PC1'] == i[0]) & (ensemble_df['PC2'] == i[1]) & (ensemble_df['PC3'] == i[2])]['structure'].values[0])\n",
    "\n",
    "cluster_best_pLDDT = []\n",
    "# for each cluster number, get the structure with the highest pLDDT\n",
    "for i in range(num_clusters):\n",
    "    cluster_best_pLDDT.append(ensemble_df[ensemble_df['cluster'] == i].sort_values(by=['pDDLDT'], ascending=False)['structure'].values[0])\n",
    "    \n",
    "# plot the cluster representatives\n",
    "#for i in cluster_representatives:\n",
    "#    plt.scatter(ensemble_df[(ensemble_df['structure'] == i)]['PC1'], ensemble_df[(ensemble_df['structure'] == i)]['PC2'], c='red', marker='x', s=100)\n",
    "\n",
    "# display the cluster representatives in the dataframe\n",
    "#print('Cluster centroids (medoids):')\n",
    "#display(ensemble_df[ensemble_df['structure'].isin(ref_structures)])\n",
    "\n",
    "# display the best structures per cluster \n",
    "print('Top pLDDT per cluster:')\n",
    "display(ensemble_df[ensemble_df['structure'].isin(cluster_best_pLDDT)].sort_values(by=['pDDLDT'], ascending=False))\n",
    "\n",
    "ref_structures = cluster_representatives\n",
    "\n",
    "# diversity pick of structures from the over\n",
    "\n",
    "xaxis = 'PC1'\n",
    "yaxis = 'PC2'\n",
    "zaxis = 'PC3'\n",
    "\n",
    "# 3D plot of the ensemble\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(ensemble_df[xaxis], ensemble_df[yaxis], ensemble_df[zaxis], c=ensemble_df['cluster'], cmap='Paired', s=1, alpha=1)\n",
    "# plot the cluster representatives\n",
    "for i in cluster_representatives:\n",
    "    ax.scatter(ensemble_df[(ensemble_df['structure'] == i)][xaxis], ensemble_df[(ensemble_df['structure'] == i)][yaxis], ensemble_df[(ensemble_df['structure'] == i)][zaxis], c='black', marker='x', s=200)\n",
    "ax.set_xlabel(xaxis)\n",
    "ax.set_ylabel(yaxis)\n",
    "ax.set_zlabel(zaxis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSDs of all structures relative to the reference structures\n",
    "\n",
    "# define function to get rmsd to a structure\n",
    "def get_rmsd_to_structure(structure):\n",
    "    mobile = mda.Universe(structure_directory + structure, structure_directory + structure) # make universe\n",
    "    mobile.atoms.residues.resids += resid_offset                                            # renumber residues in mobile\n",
    "    rmsds = alignto(mobile, ref, select=rmsd_selection, match_atoms=True, weights=None)     # these ref selections are different becasue ref has different residue numbering\n",
    "    return [structure, rmsds[1]]                                                            # [1] = rmsd after alignment\n",
    "\n",
    "# define a function to calculate the SASA of a structure\n",
    "def get_sasa(structure):\n",
    "    p = PDBParser(QUIET=1)\n",
    "    struct = p.get_structure(structure, structure_directory + structure)\n",
    "    sr = ShrakeRupley(probe_radius=1.4, n_points=100)\n",
    "    sr.compute(struct, level=\"S\")\n",
    "    return [structure, struct.sasa]  \n",
    "\n",
    "\n",
    "if use_reference_structures:\n",
    "\n",
    "    # get average positions and make new universe with these positions\n",
    "    average_positions = (u_outward_open.atoms.positions + u_inward_open.atoms.positions) / 2\n",
    "    u_average = mda.Universe(outward_open_pdb, outward_open_pdb)\n",
    "    u_average.atoms.positions = average_positions\n",
    "\n",
    "    rmsd_to_outward = {}\n",
    "\n",
    "    # outward open\n",
    "    ref = u_outward_open\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        rmsds = list(tqdm(pool.imap(get_rmsd_to_structure, structures), total=len(structures)))\n",
    "    rmsd_to_outward = dict(rmsds)   # make this list of tuples into a dictionary\n",
    "\n",
    "    rmsd_to_inward = {}\n",
    "\n",
    "    # inward open\n",
    "    ref = u_inward_open\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        rmsds = list(tqdm(pool.imap(get_rmsd_to_structure, structures), total=len(structures)))\n",
    "    rmsd_to_inward = dict(rmsds)   # make this list of tuples into a dictionary\n",
    "\n",
    "    rmsd_to_average = {}\n",
    "\n",
    "    # average\n",
    "    ref = u_average\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        rmsds = list(tqdm(pool.imap(get_rmsd_to_structure, structures), total=len(structures)))\n",
    "    rmsd_to_average = dict(rmsds)   # make this list of tuples into a dictionary\n",
    "\n",
    "    sasa_dict = {}\n",
    "\n",
    "    # solvent accesible surface areas\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        sasas = list(tqdm(pool.imap(get_sasa, structures), total=len(structures)))\n",
    "    sasa_dict = dict(sasas)         # make this list of tuples into a dictionary\n",
    "\n",
    "    # add the rmsd values to the dataframe\n",
    "    ensemble_df['rmsd_to_outward'] = rmsd_to_outward.values()\n",
    "    ensemble_df['rmsd_to_inward'] = rmsd_to_inward.values()\n",
    "    ensemble_df['rmsd_to_average'] = rmsd_to_average.values()\n",
    "    # add the sasa values to the dataframe\n",
    "    ensemble_df['sasa'] = sasa_dict.values()\n",
    "\n",
    "else:\n",
    "\n",
    "    # get rmsd with respect ot each of hte reference structures\n",
    "    for ref_structure in ref_structures:\n",
    "        \n",
    "        # make a universe with the reference structure\n",
    "        ref = mda.Universe(structure_directory + ref_structure, structure_directory + ref_structure)\n",
    "\n",
    "        # get rmsd to reference structure\n",
    "        rmsds = {}\n",
    "        with Pool(processes=num_processes) as pool:\n",
    "            rmsds = list(tqdm(pool.imap(get_rmsd_to_structure, structures), total=len(structures)))\n",
    "        rmsds = dict(rmsds)\n",
    "\n",
    "        # add to the dataframe\n",
    "        ensemble_df['rmsd_to_' + ref_structure] = rmsds.values()\n",
    "\n",
    "    sasa_dict = {}\n",
    "\n",
    "    # solvent accesible surface areas\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        sasas = list(tqdm(pool.imap(get_sasa, structures), total=len(structures)))\n",
    "    sasa_dict = dict(sasas)         # make this list of tuples into a dictionary\n",
    "\n",
    "    # add the sasa values to the dataframe\n",
    "    ensemble_df['sasa'] = sasa_dict.values()\n",
    "\n",
    "display(ensemble_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure certain distances and add them to the dataframe\n",
    "cyto_helix_bundle_1  = '(' + 'resid 286-291 or resid 349-356 or resid 299-304' + ') and name CA' # '(' + 'resid 276-305 or resid 342-357' + ') and name CA'\n",
    "cyto_helix_bundle_2  = '(' + 'resid 147-152 or resid 220-225' + ') and name CA' # '(' + 'resid 139-166 or resid 213-241' + ') and name CA'\n",
    "lumen_helix_bundle_1 = '(' + 'resid 178-182 or resid 203-207' + ') and name CA'\n",
    "lumen_helix_bundle_2 = '(' + 'resid 121-125 or resid 319-323' + ') and name CA'\n",
    "\n",
    "# calculate certain distances e.g. to use as a CV\n",
    "def calc_distance(structure, selection1, selection2):\n",
    "        universe = mda.Universe(structure_directory + structure, structure_directory + structure) # make universe\n",
    "        universe.atoms.residues.resids += resid_offset      \n",
    "        sel1_coords = universe.select_atoms(selection1).positions\n",
    "        sel2_coords = universe.select_atoms(selection2).positions\n",
    "        sel1_coords_avg = np.mean(sel1_coords, axis=0)  # geometric average = COM when all atoms are Ca\n",
    "        sel2_coords_avg = np.mean(sel2_coords, axis=0)  # geometric average = COM when all atoms are Ca\n",
    "        distance = np.linalg.norm(sel1_coords_avg - sel2_coords_avg)\n",
    "        return distance\n",
    "\n",
    "# calculate distances\n",
    "distances = {}\n",
    "for structure in structures:\n",
    "    distances[structure] = calc_distance(structure, cyto_helix_bundle_1, cyto_helix_bundle_2)\n",
    "\n",
    "# add the distances to the dataframe\n",
    "ensemble_df['cyto_helix_bundle_separation'] = distances.values()\n",
    "\n",
    "distances = {}\n",
    "for structure in structures:\n",
    "    distances[structure] = calc_distance(structure, lumen_helix_bundle_1, lumen_helix_bundle_2)\n",
    "\n",
    "# add the distances to the dataframe\n",
    "ensemble_df['lumen_helix_bundle_separation'] = distances.values()\n",
    "   \n",
    "#  show\n",
    "display(ensemble_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plots\n",
    "if use_reference_structures:\n",
    "\n",
    "    # display tables of closest to reference structures, lowest sasa, and most different\n",
    "    print('Closest to outward open:')\n",
    "    display(ensemble_df.sort_values(by=['rmsd_to_outward']).head(1)[['structure', 'pDDLDT', 'rmsd_to_outward', 'PC1', 'PC2']])\n",
    "    print('Closest to inward open:')\n",
    "    display(ensemble_df.sort_values(by=['rmsd_to_inward']).head(1)[['structure', 'pDDLDT', 'rmsd_to_inward', 'PC1', 'PC2']])\n",
    "\n",
    "    # plot the rmsd to the reference structures\n",
    "    plt.scatter(ensemble_df['rmsd_to_outward'], ensemble_df['rmsd_to_inward'], c=ensemble_df['pDDLDT'], cmap='coolwarm')\n",
    "    plt.xlabel('RMSD to outward open')\n",
    "    plt.ylabel('RMSD to inward open')\n",
    "    plt.colorbar(label='pLDDT')\n",
    "    #limits\n",
    "    plt.xlim(0)\n",
    "    plt.ylim(0)\n",
    "    plt.show()\n",
    "\n",
    "        # plot the rmsd to the reference structures\n",
    "    plt.scatter(ensemble_df['rmsd_to_outward'], ensemble_df['rmsd_to_inward'], c=ensemble_df['sasa'], cmap='coolwarm')\n",
    "    plt.xlabel('RMSD to outward open')\n",
    "    plt.ylabel('RMSD to inward open')\n",
    "    plt.colorbar(label='SASA')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # plot histogram of rmsd to outward open\n",
    "    plt.hist(ensemble_df['rmsd_to_outward'], bins=20, alpha=0.5, label='outward open')\n",
    "    plt.hist(ensemble_df['rmsd_to_inward'], bins=20, alpha=0.5, label='inward open')\n",
    "\n",
    "else:\n",
    "\n",
    "    # display tables of closest to reference structures, lowest sasa, and most different\n",
    "    print('Represnetative structures from each cluster:')\n",
    "    for i in ref_structures:\n",
    "        display(ensemble_df.sort_values(by=['rmsd_to_' + i]).head(1)[['structure', 'pDDLDT', 'PC1', 'PC2']])\n",
    "    print('Lowest 3 SASA:')\n",
    "    display(ensemble_df.sort_values(by=['sasa']).head(3)[['structure', 'cluster', 'pDDLDT', 'PC1', 'PC2']])\n",
    "    \n",
    "    \n",
    "    # plot ensemble\n",
    "    plt.scatter(ensemble_df[plot_variable_1], ensemble_df[plot_variable_2], c=ensemble_df[plot_variable_3], cmap='coolwarm')\n",
    "    plt.xlabel(plot_variable_1)\n",
    "    plt.ylabel(plot_variable_2)\n",
    "    plt.colorbar(label=plot_variable_3)\n",
    "    \n",
    "    # annotate reference structures\n",
    "    for i in ref_structures:\n",
    "        plt.scatter(ensemble_df.loc[ensemble_df['structure'] == i][plot_variable_1], ensemble_df.loc[ensemble_df['structure'] == i][plot_variable_2], c='black', marker='x', s=100)\n",
    "        plt.annotate(i, (ensemble_df.loc[ensemble_df['structure'] == i][plot_variable_1], ensemble_df.loc[ensemble_df['structure'] == i][plot_variable_2]))\n",
    "    plt.show()\n",
    "    \n",
    "    # for each reference structure, plot the histogram of rmsd to that structure\n",
    "    for i in ref_structures:\n",
    "        plt.hist(ensemble_df['rmsd_to_' + i], bins=20, alpha=0.5, label=i)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('RMSD')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlim(0)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10, 2)\n",
    "    plt.show()\n",
    "    \n",
    "    # plot 2 pane histogram of SASA and pLDDT scores\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.hist(ensemble_df['sasa'], bins=20, alpha=0.5, label='sasa', color='C3')\n",
    "    ax1.set_xlabel('SASA')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax2.hist(ensemble_df['pDDLDT'], bins=20, alpha=0.5, label='pDDLDT')\n",
    "    ax2.set_xlabel('pDDLDT')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10, 2)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel calculation of the pairwise RMSD matrix (for the MC energy function used in the next cell)\n",
    "\n",
    "## load backup matrix\n",
    "rmsd_matrix = np.load(base_directory + 'rmsd_matrix_backup_90_bigsample.npy') # 'rmsd_matrix_80_6_0_3326-strucs.npy'\n",
    "\n",
    "# # Initialize a matrix to store pairwise RMSD values\n",
    "# rmsd_matrix = np.zeros((len(ensemble_df), len(ensemble_df)))\n",
    "# \n",
    "# # function to calculate RMSD values in parallel\n",
    "# def calculate_rmsd(i):\n",
    "# \n",
    "#     rmsd_values = []\n",
    "# \n",
    "#     # setup universe for i-th structure\n",
    "#     mobile = mda.Universe(structure_directory + ensemble_df['structure'].values[i], structure_directory + ensemble_df['structure'].values[i])\n",
    "#     if mobile.atoms.residues.resids[0] == 1: # if first resID == one, then residues are already numbered correctly\n",
    "#         mobile.atoms.residues.resids += resid_offset\n",
    "# \n",
    "#     for j in range(len(ensemble_df)):\n",
    "# \n",
    "#         # setup universe for j-th structure\n",
    "#         ref = mda.Universe(structure_directory + ensemble_df['structure'].values[j], structure_directory + ensemble_df['structure'].values[j])\n",
    "#         if ref.atoms.residues.resids[0] == 1: # if first resID == one, then residues are already numbered correctly\n",
    "#             ref.atoms.residues.resids += resid_offset\n",
    "# \n",
    "#         # Calculate RMSD and store it in the list\n",
    "#         rmsd = alignto(mobile, ref, select=rmsd_selection, match_atoms=True)[1]\n",
    "#         rmsd_values.append(rmsd)\n",
    "# \n",
    "#     mobile = None       # Release memory by setting the loaded structures to None\n",
    "#     ref = None\n",
    "# \n",
    "#     return rmsd_values\n",
    "# \n",
    "# # Perform parallel computation of RMSD values\n",
    "# with Pool(processes=num_processes) as pool:\n",
    "#     results = list(tqdm(pool.imap(calculate_rmsd, range(len(ensemble_df))), total=len(ensemble_df)))\n",
    "# \n",
    "# # Update the rmsd_matrix with the results\n",
    "# for i, rmsd_values in enumerate(results):\n",
    "#     rmsd_matrix[i] = rmsd_values\n",
    "# \n",
    "# results = None  # Release memory by setting the results to None\n",
    "# \n",
    "# np.save(base_directory + 'rmsd_matrix.npy', rmsd_matrix)    # save the matrix (to avoid recomputing it in the future)\n",
    "# \n",
    "# # Display progress information\n",
    "# print('Calculation completed for', len(ensemble_df), 'pairs of structures', end='\\r')\n",
    "# \n",
    "# plot the matrix\n",
    "plt.imshow(rmsd_matrix, cmap='inferno')\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate faster way to calculate rmsd matrix\n",
    "\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import jax\n",
    "from Bio.PDB import PDBParser\n",
    "\n",
    "def pdb_to_coordinates(pdb_file):\n",
    "    parser = PDBParser()\n",
    "    structure = parser.get_structure('trajectory', pdb_file)\n",
    "    # Initialize an empty list to store coordinates\n",
    "    coordinates = []\n",
    "    for model in structure:\n",
    "        model_coordinates = []\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                for atom in residue:\n",
    "                    # Extract coordinates\n",
    "                    atom_coordinates = atom.get_coord()\n",
    "                    model_coordinates.append(atom_coordinates)\n",
    "        coordinates.append(model_coordinates)\n",
    "    # Convert the list of coordinates into a numpy array\n",
    "    return np.array(coordinates)\n",
    "# Read trajectory from a PDB file\n",
    "trajectory_coordinates = pdb_to_coordinates(base_directory + 'ensemble.pdb')\n",
    "# Print shape of the numpy array\n",
    "print(\"Shape of the numpy array:\", trajectory_coordinates.shape)\n",
    "def jnp_kabsch(a, b):\n",
    "  u, s, vh = jnp.linalg.svd(a.T @ b, full_matrices=False)\n",
    "  u = jnp.where(jnp.linalg.det(u @ vh) < 0, u.at[:,-1].set(-u[:,-1]), u)\n",
    "  return u @ vh\n",
    "def jnp_rmsd(true, pred):\n",
    "  p = true - true.mean(0,keepdims=True)\n",
    "  q = pred - pred.mean(0,keepdims=True)\n",
    "  p = p @ jnp_kabsch(p, q)\n",
    "  return jnp.sqrt(jnp.square(p-q).sum(-1).mean())\n",
    "jnp_rmsd_parallel = jax.jit(jax.vmap(jnp_rmsd,(None,0)))\n",
    "data = trajectory_coordinates\n",
    "#%%time\n",
    "\n",
    "results = []\n",
    "for n in range(len(ensemble_df)):\n",
    "  results.append(np.array(jnp_rmsd_parallel(data[n],data)))\n",
    "\n",
    "# turn the \"results\" list into a numpy array float 64\n",
    "results = np.array(results, dtype=np.float64)\n",
    "\n",
    "# declare the rmsd_matrix the one here\n",
    "rmsd_matrix = results\n",
    "\n",
    "# plot the results as a matrix with heatmap\n",
    "plt.imshow(rmsd_matrix, cmap='inferno')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a matrix of pairwise SASA differences\n",
    "sasa_matrix = np.zeros((len(ensemble_df), len(ensemble_df)))\n",
    "n_elements = len(ensemble_df) * len(ensemble_df)\n",
    "\n",
    "# all pairwise sasas (these can just be subtracted from eachother)\n",
    "for i in range(len(ensemble_df)):\n",
    "    for j in range(len(ensemble_df)):\n",
    "        sasa_matrix[i,j] = (ensemble_df['sasa'].iloc[i] - ensemble_df['sasa'].iloc[j])\n",
    "        #sasa_matrix[i,j] = (ensemble_df['sasa'][i] - ensemble_df['sasa'][j])\n",
    "\n",
    "# show the matrix\n",
    "plt.imshow(sasa_matrix, cmap='coolwarm')\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a matrix of differences in collective variable between structures\n",
    "cv_matrix = np.zeros((len(ensemble_df), len(ensemble_df)))\n",
    "n_elements = len(ensemble_df) * len(ensemble_df)\n",
    "\n",
    "# all pairwise difference in collective variable (these can just be subtracted from eachother)\n",
    "for i in range(len(ensemble_df)):\n",
    "    for j in range(len(ensemble_df)):\n",
    "        cv_matrix[i,j] = (ensemble_df[collective_variable].iloc[i] - ensemble_df[collective_variable].iloc[j])\n",
    "        #cv_matrix[i,j] = (ensemble_df[collective_variable][i] - ensemble_df[collective_variable][j])\n",
    "\n",
    "# show the matrix\n",
    "plt.imshow(cv_matrix, cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign bins for binned mc path finding\n",
    "\n",
    "use_defined_end_states = True\n",
    "\n",
    "occluded_state = 'msa-16-32_rank_024.pdb'\n",
    "#lumen_open 'msa-128-256_rank_1163.pdb'\n",
    "#cyto open 'msa-32-64_rank_127.pdb'\n",
    "\n",
    "# end states to interpolate between values of the CV of\n",
    "end_state_1 = 'msa-32-64_rank_127.pdb' #'' #msa-128-256_rank_1163.pdb\n",
    "end_state_2 = occluded_state\n",
    "\n",
    "# get the values of the CV for these structures\n",
    "end_state_1_cv_value = ensemble_df[ensemble_df['structure'] == end_state_1][collective_variable].values[0]\n",
    "end_state_2_cv_value = ensemble_df[ensemble_df['structure'] == end_state_2][collective_variable].values[0]\n",
    "\n",
    "if use_defined_end_states == True:\n",
    "    max_value = end_state_1_cv_value\n",
    "    min_value = end_state_2_cv_value\n",
    "else:\n",
    "    # just get the min and max value of the end states globally, like before\n",
    "    max_value = ensemble_df[collective_variable].max()\n",
    "    min_value = ensemble_df[collective_variable].min()\n",
    "\n",
    "\n",
    "## get the ideal collective variable values per window between min and max_value using linear interpolation\n",
    "ideal_window_values = np.zeros(mc_n_bins)\n",
    "for i in range(mc_n_bins):\n",
    "    ideal_window_values[i] = np.interp(i, [0, mc_n_bins-1], [max_value, min_value])\n",
    "ideal_initial_path = []\n",
    "for i in ideal_window_values:\n",
    "    ideal_initial_path.append(ensemble_df[collective_variable].sub(i).abs().idxmin())\n",
    "\n",
    "# replace the first and last structures in the ideal path with the end states\n",
    "if use_defined_end_states == True:\n",
    "    ideal_initial_path[0] = ensemble_df[ensemble_df['structure'] == end_state_1].index[0]\n",
    "    ideal_initial_path[-1] = ensemble_df[ensemble_df['structure'] == end_state_2].index[0]\n",
    "\n",
    "## set up the bins for the binned path\n",
    "ensemble_df['closest_ideal_structure'] = ensemble_df[collective_variable].apply(lambda x: ideal_initial_path[np.argmin(np.abs(ideal_window_values - x))])\n",
    "ensemble_df['bin'] = ensemble_df['closest_ideal_structure'].apply(lambda x: ideal_initial_path.index(x))\n",
    "\n",
    "\n",
    "# plotting\n",
    "#display(ensemble_df)\n",
    "plt.hist(ensemble_df['bin'], bins=mc_n_bins)\n",
    "plt.show()\n",
    "\n",
    "# show bin distribution\n",
    "plt.hexbin(ensemble_df[plot_variable_1], ensemble_df[plot_variable_2], C=ensemble_df['bin'], cmap='tab20c', gridsize=40, reduce_C_function=np.amax)\n",
    "#plt.scatter(ensemble_df[plot_variable_1], ensemble_df[plot_variable_2], c=ensemble_df['bin'], cmap='tab20c')\n",
    "plt.colorbar()\n",
    "\n",
    "# plot the initial guesses\n",
    "for i in range(mc_n_bins):\n",
    "    plt.scatter(ensemble_df.loc[ensemble_df.index == ideal_initial_path[i]][plot_variable_1], ensemble_df.loc[ensemble_df.index == ideal_initial_path[i]][plot_variable_2], marker='x', color='black', s=10)\n",
    "# label the ideal structures wit htheir associated bin\n",
    "for i in range(mc_n_bins):\n",
    "    plt.annotate(i, (ensemble_df.loc[ensemble_df.index == ideal_initial_path[i]][plot_variable_1], ensemble_df.loc[ensemble_df.index == ideal_initial_path[i]][plot_variable_2]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC path finding by exchanging structures within bins\n",
    "\n",
    "def calc_energy(path, path_length):\n",
    "\n",
    "    wf_rmsd = 1\n",
    "    wf_cv = 1\n",
    "\n",
    "    # for minimising rmsd between structures on path\n",
    "    energy_rmsd = 0\n",
    "    for i in range(path_length - 1):\n",
    "        energy_rmsd += (rmsd_matrix[np.where(ensemble_df.index.values == path[i])[0][0], np.where(ensemble_df.index.values == path[i+1])[0][0]]**2)\n",
    "    energy_rmsd = wf_rmsd * np.sqrt ( 1/path_length * energy_rmsd )\n",
    "\n",
    "    # for minimising pairwise differences in collective variable between structures on path\n",
    "    energy_cv = 0\n",
    "    for i in range(path_length -1):\n",
    "        energy_cv += (cv_matrix[np.where(ensemble_df.index.values == path[i])[0][0], np.where(ensemble_df.index.values == path[i+1])[0][0]]**2)\n",
    "    energy_cv = wf_cv * np.sqrt ( 1/path_length * energy_cv )\n",
    "\n",
    "    # final term is sum\n",
    "    total_energy = 0\n",
    "    total_energy = energy_rmsd + energy_cv\n",
    "\n",
    "    # final energy\n",
    "    return total_energy\n",
    "\n",
    "# define a function that runs monte carlo simulated annealing to optimise path smoothness\n",
    "def mc_path_optimisation(seed):\n",
    "\n",
    "    fixed_endpoints = True\n",
    "    mc_n_steps = 1000\n",
    "    initial_temperature = 0.01\n",
    "    cooling_factor = 10000 # initial temp is divided by this to get the final temperature\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # list of temperatures mapped to mc_n_steps decreasing stepwise in 10 increments\n",
    "    final_temperature = initial_temperature/cooling_factor\n",
    "    temperatures = np.logspace(np.log10(initial_temperature), np.log10(final_temperature), num=mc_n_steps)\n",
    "\n",
    "    mc_path_length = mc_n_bins\n",
    "\n",
    "    mcpath = []\n",
    "\n",
    "    initial_guess_indices = []\n",
    "    initial_guess_indices = ideal_initial_path\n",
    "\n",
    "    mcpath = initial_guess_indices\n",
    "\n",
    "    # dictionary of paths and energies\n",
    "    path_energies = {}\n",
    "\n",
    "    # run n_mc_runs steps\n",
    "    for i in range(mc_n_steps):\n",
    "\n",
    "        temperature = temperatures[i]\n",
    "\n",
    "        # calculate energy of the initial path\n",
    "        energy = 0\n",
    "        energy = calc_energy(mcpath, mc_path_length)\n",
    "        \n",
    "        # propose an exchange of a random (not endstate) structure in the path with a random structure from the pool\n",
    "        new_mcpath = mcpath.copy()\n",
    "\n",
    "        if fixed_endpoints == True:\n",
    "            start_point = 1\n",
    "            end_point = len(new_mcpath)-1\n",
    "        else:\n",
    "            start_point = 0\n",
    "            end_point = len(new_mcpath)\n",
    "\n",
    "        # loop throguh new_mcpath\n",
    "        for point in range(start_point, end_point):\n",
    "\n",
    "            new_mcpath = mcpath.copy()\n",
    "            \n",
    "            structure_in_path = mcpath[point]\n",
    "\n",
    "            # select a random structure in the same bin as the structure in the path\n",
    "            random_structure_in_pool = np.random.choice(ensemble_df[ensemble_df['bin'] == ensemble_df.loc[structure_in_path]['bin']].index.values)\n",
    "\n",
    "            # replace the random structure in the new path with the random structure from the pool\n",
    "            new_mcpath[np.where(new_mcpath == structure_in_path)[0][0]] = random_structure_in_pool\n",
    "\n",
    "            # calculate the energy of the original path\n",
    "            energy = 0\n",
    "            energy = calc_energy(mcpath, mc_path_length)\n",
    "\n",
    "            # calcualte energy of proposed path\n",
    "            new_energy = 0\n",
    "            new_energy = calc_energy(new_mcpath, mc_path_length)\n",
    "\n",
    "            # calculate the difference between the two energies\n",
    "            delta_energy = 0\n",
    "            delta_energy = new_energy - energy\n",
    "\n",
    "            # metropolis criterion\n",
    "\n",
    "            # if lower, accept the new path\n",
    "            if delta_energy < 0:\n",
    "                descision = 'accepted'\n",
    "                mcpath = new_mcpath\n",
    "                path_energies[new_energy] = mcpath\n",
    "            else:\n",
    "            \n",
    "                # if higher, accept with probability e^(-difference/temp)\n",
    "                if np.random.rand() < np.exp(-delta_energy / temperature):\n",
    "                    descision = 'accepted'\n",
    "                    mcpath = new_mcpath\n",
    "                    path_energies[new_energy] = mcpath\n",
    "                else:\n",
    "                    descision = 'rejected'\n",
    "                    pass\n",
    "            \n",
    "            # do not uncomment this when running in parallel, will break your editor\n",
    "            #print('dE:',delta_energy,' T:', temperature, ' rand:', np.exp(-delta_energy / temperature), descision)\n",
    "            \n",
    "    # get the path with the lowest energy\n",
    "    final_energy = min(path_energies.keys())\n",
    "    final_path = path_energies[final_energy]\n",
    "    relaxation_energies = list(path_energies.keys())\n",
    "    final_path_structures = tuple(ensemble_df.loc[final_path]['structure'].values)\n",
    "\n",
    "    return [final_energy, final_path, final_path_structures, relaxation_energies]\n",
    "\n",
    "#  run once for testing\n",
    "#  random seed\n",
    "# seed = np.random.randint(0, 1000000)\n",
    "# mc_runs = []\n",
    "# mc_runs.append(mc_path_optimisation(seed))\n",
    "\n",
    "mc_n_runs = 100\n",
    "\n",
    "# run mc_n_bins runs in parallel\n",
    "\n",
    "# re-randomised, nonreproduciible version\n",
    "# seeds = np.random.randint(low=0, high=1e3, size=mc_n_runs)  # Create a list of random seeds\n",
    "# with Pool(processes=num_processes) as pool:\n",
    "#     mc_runs = list(tqdm(pool.imap(mc_path_optimisation, zip(range(mc_n_runs), seeds)), total=mc_n_runs))\n",
    "\n",
    "# reproducible version\n",
    "with Pool(processes=num_processes) as pool:\n",
    "    mc_runs = list(tqdm(pool.imap(mc_path_optimisation, range(mc_n_runs)), total=mc_n_runs))\n",
    "\n",
    "# add the final energies and final path structures to a dataframe\n",
    "mc_runs_df = pd.DataFrame(mc_runs, columns=['energy', 'path', 'path structures', 'relaxation energies'])\n",
    "mc_runs_df = mc_runs_df.sort_values(by=['energy']) \n",
    "\n",
    "# display the best run\n",
    "display(mc_runs_df[['energy', 'path structures']].head(3))\n",
    "\n",
    "# plot relaxation of the best run\n",
    "for i in range(1, 4):\n",
    "    plt.plot(mc_runs_df.iloc[i]['relaxation energies'])\n",
    "\n",
    "# label\n",
    "plt.xlabel('MC step')\n",
    "plt.ylabel('Energy')\n",
    "plt.show()\n",
    "\n",
    "# calculate the energy for each point in the best path (mc_runs[1]) (for plotting)\n",
    "energy_over_path = []\n",
    "energy = {}\n",
    "\n",
    "# get hte \"path\" of the best run (in indices)\n",
    "test = mc_runs_df.iloc[0]['path']\n",
    "for i in range(mc_n_bins - 1):\n",
    "        energy[i] = (rmsd_matrix[np.where(ensemble_df.index.values == test[i])[0][0], np.where(ensemble_df.index.values == test[i+1])[0][0]]**2)\n",
    "        energy[i] = np.sqrt ( 1 * energy[i] )\n",
    "plt.plot(energy.values(), label='best')\n",
    "test = mc_runs_df.iloc[-1]['path']\n",
    "for i in range(mc_n_bins - 1):\n",
    "        energy[i] = (rmsd_matrix[np.where(ensemble_df.index.values == test[i])[0][0], np.where(ensemble_df.index.values == test[i+1])[0][0]]**2)\n",
    "        energy[i] = np.sqrt ( 1 * energy[i] )\n",
    "plt.plot(energy.values(), label='worst')\n",
    "# plot energy\n",
    "\n",
    "# plot energy\n",
    "plt.legend()\n",
    "# label axes\n",
    "plt.xlabel('point in path')\n",
    "plt.ylabel('MC energy')\n",
    "plt.show()\n",
    "\n",
    "# write out the best run to a multistate pdb file and append timestamp to filename\n",
    "with mda.Writer(base_directory + 'best_run.pdb', u.atoms.n_atoms) as W:\n",
    "    # get the structures of the lowest energy path\n",
    "    for structure in mc_runs_df.head(1)['path structures'].values[0]:\n",
    "        # if structure is the reference, dont renumber residues\n",
    "        if structure == 'ref_outward.pdb' or structure == 'ref_inward.pdb':\n",
    "            # use a non-reference structure to make the universe\n",
    "            u = mda.Universe(structure_directory + structure, \n",
    "                             structure_directory + structure) # make universe\n",
    "            u.atoms.residues.resids += resid_offset \n",
    "            u.atoms.segments.segids = 'A'\n",
    "            u.atoms.chainIDs = 'A'\n",
    "            W.write(u.select_atoms('protein'))\n",
    "        else:\n",
    "            u = mda.Universe(structure_directory + structure, \n",
    "                             structure_directory + structure) # make universe\n",
    "            u.atoms.residues.resids += resid_offset\n",
    "            W.write(u.select_atoms('protein'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the best path\n",
    "\n",
    "# plot all the structures\n",
    "plt.hexbin(ensemble_df[plot_variable_1], ensemble_df[plot_variable_2], C=ensemble_df['bin'], cmap='tab20c', gridsize=40, reduce_C_function=np.amax)\n",
    "#\n",
    "#plt.scatter(ensemble_df[plot_variable_1], ensemble_df[plot_variable_2], c=ensemble_df['bin'], cmap='tab20c', alpha=0.8, s=2)\n",
    "# plot all the structures as a hexgrid \n",
    "\n",
    "\n",
    "# for each structure in best run, plot the rmsd to outward and inward from the ensemble_df dataframe\n",
    "for structure in mc_runs_df.head(1)['path'].values[0]:\n",
    "    plt.scatter(ensemble_df.loc[structure][plot_variable_1], ensemble_df.loc[structure][plot_variable_2], c='black', s=32)\n",
    "    # annotate them with their bin number\n",
    "    #plt.annotate(ensemble_df.loc[structure]['bin'], (ensemble_df.loc[structure][plot_variable_1], ensemble_df.loc[structure][plot_variable_2]), xytext=(ensemble_df.loc[structure][plot_variable_1], ensemble_df.loc[structure][plot_variable_2]+0.5), ha='center', va='bottom', arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.2'))\n",
    "\n",
    "# draw connecting lines\n",
    "for i in range(len(mc_runs_df.head(1)['path'].values[0]) - 1):\n",
    "    # get the rmsd to outward and inward from ensemble_df\n",
    "    x1 = ensemble_df.loc[mc_runs_df.head(1)['path'].values[0][i]][plot_variable_1]\n",
    "    y1 = ensemble_df.loc[mc_runs_df.head(1)['path'].values[0][i]][plot_variable_2]\n",
    "    x2 = ensemble_df.loc[mc_runs_df.head(1)['path'].values[0][i+1]][plot_variable_1]\n",
    "    y2 = ensemble_df.loc[mc_runs_df.head(1)['path'].values[0][i+1]][plot_variable_2]\n",
    "    plt.plot([x1, x2], [y1, y2], c='black', alpha=0.6, linestyle='dashed')\n",
    "\n",
    "# plot the initial guesses\n",
    "#for i in range(mc_n_bins):\n",
    "#    plt.scatter(ensemble_df.loc[ensemble_df.index == ideal_initial_path[i]][plot_variable_1], ensemble_df.loc[ensemble_df.index == ideal_initial_path[i]][plot_variable_2], marker='x', color='black', s=10)\n",
    "\n",
    "#display(mc_runs_df.head(1))\n",
    "#display(mc_runs_df.tail(1))\n",
    "\n",
    "plt.xlabel(plot_variable_1)\n",
    "plt.ylabel(plot_variable_2)\n",
    "plt.show()\n",
    "\n",
    "# Plot how good this is\n",
    "\n",
    "# plot the ideal window values\n",
    "plt.plot(ideal_window_values, label='ideal', color='black', alpha=0.2)\n",
    "# show dots\n",
    "plt.scatter(range(mc_n_bins), ideal_window_values, c=range(mc_n_bins), cmap='coolwarm', alpha=0.5)\n",
    "\n",
    "# get the actual values\n",
    "actual_window_values = np.zeros(mc_n_bins)\n",
    "for i in range(mc_n_bins):\n",
    "    actual_window_values[i] = ensemble_df.loc[mc_runs_df.head(1)['path'].values[0][i]][collective_variable]\n",
    "\n",
    "## print the ideal and actual values in a table (rounded to 2 decimal places)\n",
    "#print(pd.DataFrame({'ideal': ideal_window_values, 'actual': actual_window_values}).round(2))\n",
    "\n",
    "# plot the actual values\n",
    "plt.plot(actual_window_values, label='actual', c='black', alpha=0.8)\n",
    "# show dots\n",
    "plt.scatter(range(mc_n_bins), actual_window_values, c=range(mc_n_bins), cmap='coolwarm')\n",
    "\n",
    "# plot vertical lines between the two\n",
    "for i in range(mc_n_bins):\n",
    "    plt.plot([i, i], [ideal_window_values[i], actual_window_values[i]], c='black', alpha=0.2, linestyle='dashed')\n",
    "\n",
    "if ideal_initial_path == True:\n",
    "\n",
    "    # alternate where initial guessi ncides are just those of the structures closest to the ideal values of the collective variable (from a linear interpolation)\n",
    "    initial_guess_indices = []\n",
    "    # get the ideal values of the collective variable\n",
    "\n",
    "    # enpoint values are min adn max of the collective variable\n",
    "    ideal_values = np.linspace(ensemble_df[collective_variable].min(), ensemble_df[collective_variable].max(), mc_n_bins)\n",
    "\n",
    "    # get the closest structure to each ideal value\n",
    "    for i in ideal_values:\n",
    "        initial_guess_indices.append(ensemble_df[collective_variable].sub(i).abs().idxmin())\n",
    "    # plot the initial guesses\n",
    "    for i in range(mc_n_bins):\n",
    "        plt.scatter(i, ensemble_df.loc[initial_guess_indices[i]][collective_variable], marker='x', color='black')\n",
    "    print(len(initial_guess_indices))   \n",
    "\n",
    "# x axis labels\n",
    "plt.xticks(range(mc_n_bins), range(mc_n_bins))\n",
    "plt.xlabel('Window Number')\n",
    "plt.ylabel(collective_variable + 'CV')\n",
    "plt.legend()\n",
    "\n",
    "# add a title\n",
    "plt.title('Ideal vs Actual Window CV Values')\n",
    "plt.show()\n",
    "\n",
    "# now just plot the offsets (absolute) as a bar chart\n",
    "offsets = np.abs(ideal_window_values - actual_window_values)\n",
    "plt.bar(range(mc_n_bins), offsets, color='black', alpha=0.2)\n",
    "plt.xlabel('Window Number')\n",
    "plt.ylabel('Absolute Offset')\n",
    "plt.title('how good is the path?')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up umbrella sampling windows (for REUS)\n",
    "from MDAnalysis.analysis import align, rms\n",
    "import shutil\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def write_pca_ref_pdb():\n",
    "\n",
    "    # write a file like pca1.pdb but with \"REMARK TYPE=OPTIMAL\" after each ENDMDL line\n",
    "    # edit this file to add \"REMARK TYPE=OPTIMAL\" after each ENDMDL line\n",
    "    with open(umbrella_sampling_directory + 'pca_ref.pdb', 'r') as file :\n",
    "        filedata = file.read()\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('ENDMDL', 'ENDMDL\\nREMARK TYPE=OPTIMAL')\n",
    "    # Write the file out again\n",
    "    with open(umbrella_sampling_directory + 'pca_ref.pdb', 'w') as file:\n",
    "        file.write(filedata)\n",
    "\n",
    "# function to calculate the similarity between two strings\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "# this should be a pdb from e.g. an unbiased simulation you've run before\n",
    "# containing everything in the simualtion box, protein, lipids, water etc. \n",
    "# if you have a working topolgy for this system, you can re-use it here\n",
    "template_pdb_for_umbrella_sampling = (base_directory + 'template.pdb')\n",
    "\n",
    "# make a universe from this template\n",
    "u_template = mda.Universe(template_pdb_for_umbrella_sampling, template_pdb_for_umbrella_sampling)\n",
    "\n",
    "# sort out your collective variable, for later inclusion in the\n",
    "# automatically generated plumed input file\n",
    "print('collective variable is:', collective_variable, '\\n')\n",
    "if collective_variable == 'cyto_helix_bundle_separation':\n",
    "    com1 = cyto_helix_bundle_1\n",
    "    com2 = cyto_helix_bundle_2\n",
    "    pcavar = False\n",
    "elif collective_variable == 'lumen_helix_bundle_separation':\n",
    "    com1 = lumen_helix_bundle_1\n",
    "    com2 = lumen_helix_bundle_2\n",
    "    pcavar = False\n",
    "elif collective_variable == 'PC1':\n",
    "    com1 = None\n",
    "    com2 = None\n",
    "    pcavar = True\n",
    "    write_pca_ref_pdb()\n",
    "    pca_ref_pdb = umbrella_sampling_directory + 'pca_ref.pdb'\n",
    "else:\n",
    "    print('I dont know what your collective variable is supposed to be, sort this out yourself (thinking emoji)')\n",
    "    com1 = '{your atoms here}'\n",
    "    com2 = '{your atoms here}'\n",
    "    pcavar = False\n",
    "\n",
    "\n",
    "# align a mobile universe to a reference universe\n",
    "def align_universe(mobile, ref):\n",
    "# aligns mobile universe to first frame of reference universe\n",
    "    alignment_selection = 'protein and name CA'\n",
    "    mobile.trajectory[-1]  # set mobile trajectory to last frame\n",
    "    ref.trajectory[0]  # set reference trajectory to first frame\n",
    "    mobile_ca = mobile.select_atoms(alignment_selection)\n",
    "    ref_ca = ref.select_atoms(alignment_selection)\n",
    "    rms.rmsd(\n",
    "             mobile_ca.positions, \n",
    "             ref_ca.positions, \n",
    "             superposition=False\n",
    "             )\n",
    "    aligner = align.AlignTraj(mobile, \n",
    "                              ref, \n",
    "                              select=alignment_selection,\n",
    "                              in_memory=True).run()\n",
    "    return mobile\n",
    "\n",
    "# replace the protein coordinates in your template with those from a structure in the ensemble\n",
    "def protein_coordinate_replacer(u_structure, u_template):\n",
    "\n",
    "    # dictionary of name matches for charmm36 (and I assume amber)\n",
    "    # key is the name in the template universe, value is the name in the structure universe\n",
    "    name_mismatches = {\n",
    "    'ALA HN'   : 'H',\n",
    "    'ARG HB1'  : 'HB2',\n",
    "    'ARG HB2'  : 'HB3',\n",
    "    'ARG HD1'  : 'HD2',\n",
    "    'ARG HD2'  : 'HD3',\n",
    "    'ARG HG1'  : 'HG2',\n",
    "    'ARG HG2'  : 'HG3',\n",
    "    'ARG HN'   : 'H',\n",
    "    'ASN HB1'  : 'HB2',\n",
    "    'ASN HB2'  : 'HB3',\n",
    "    'ASN HN'   : 'H',\n",
    "    'ASP HB1'  : 'HB2',\n",
    "    'ASP HB2'  : 'HB3',\n",
    "    'ASP HN'   : 'H',\n",
    "    'CYS HB1'  : 'HB2',\n",
    "    'CYS HB2'  : 'HB3',\n",
    "    'CYS HG1'  : 'HG',\n",
    "    'CYS HN'   : 'H',\n",
    "    'GLN HB1'  : 'HB2',\n",
    "    'GLN HB2'  : 'HB3',\n",
    "    'GLN HG1'  : 'HG2',\n",
    "    'GLN HG2'  : 'HG3',\n",
    "    'GLN HN'   : 'H',\n",
    "    'GLU HB1'  : 'HB2',\n",
    "    'GLU HB2'  : 'HB3',\n",
    "    'GLU HG1'  : 'HG2',\n",
    "    'GLU HG2'  : 'HG3',\n",
    "    'GLU HN'   : 'H',\n",
    "    'GLY HA1'  : 'HA2',\n",
    "    'GLY HA2'  : 'HA3',\n",
    "    'GLY HN'   : 'H',\n",
    "    'HSD HB1'  : 'HB2',\n",
    "    'HSD HB2'  : 'HB3',\n",
    "    'HSD HN'   : 'H',\n",
    "    'LEU HN'   : 'H',\n",
    "    'LYS HB1'  : 'HB2',\n",
    "    'LYS HB2'  : 'HB3',   \n",
    "    'LYS HD1'  : 'HD2',\n",
    "    'LYS HD2'  : 'HD3',\n",
    "    'LYS HE1'  : 'HE2',\n",
    "    'LYS HE2'  : 'HE3',\n",
    "    'LYS HG1'  : 'HG2',\n",
    "    'LYS HG2'  : 'HG3',\n",
    "    'LYS HN'   : 'H',\n",
    "    'MET HB1'  : 'HB2',\n",
    "    'MET HB2'  : 'HB3',\n",
    "    'MET HG1'  : 'HG2',\n",
    "    'MET HG2'  : 'HG3',\n",
    "    'MET HN'   : 'H',\n",
    "    'PHE HB1'  : 'HB2',\n",
    "    'PHE HB2'  : 'HB3',\n",
    "    'PHE HN'   : 'H',\n",
    "    'PRO HB1'  : 'HB2',\n",
    "    'PRO HB2'  : 'HB3',\n",
    "    'PRO HD1'  : 'HD2',\n",
    "    'PRO HD2'  : 'HD3',\n",
    "    'PRO HG1'  : 'HG2',\n",
    "    'PRO HG2'  : 'HG3',\n",
    "    'SER HB1'  : 'HB2',\n",
    "    'SER HB2'  : 'HB3',\n",
    "    'SER HG1'  : 'HG',\n",
    "    'SER HN'   : 'H',\n",
    "    'THR HN'   : 'H',\n",
    "    'TRP HB1'  : 'HB2',\n",
    "    'TRP HB2'  : 'HB3',\n",
    "    'TRP HN'   : 'H',\n",
    "    'TYR HB1'  : 'HB2',\n",
    "    'TYR HB2'  : 'HB3',\n",
    "    'TYR HN'   : 'H',\n",
    "    'ILE CD'   : 'CD1',\n",
    "    'ILE HD1'  : 'HD11',\n",
    "    'ILE HD2'  : 'HD12',\n",
    "    'ILE HD3'  : 'HD13', \n",
    "    'ILE HG11' : 'HG12',\n",
    "    'ILE HG12' : 'HG13',\n",
    "    'ILE HN'   : 'H',\n",
    "    'LEU HB1' : 'HB2',\n",
    "    'LEU HB1' : 'HB2',\n",
    "    'VAL HN'  : 'H',\n",
    "    }\n",
    "\n",
    "\n",
    "    error_counter = 0\n",
    "    error_type_list = []\n",
    "\n",
    "    # renumber the residues in the structure universe\n",
    "    u_structure.atoms.residues.resids += resid_offset\n",
    "\n",
    "    # dict to keep track of which atoms in the structure universe have been accounted for (residue: atom)\n",
    "    modified_atoms = []\n",
    "\n",
    "    # loop though each atom in the template universe matching the atom name and residue ID to the atom in the template universe\n",
    "    for atom in u_template.select_atoms('protein').atoms:\n",
    "\n",
    "        atom_name_template = atom.name\n",
    "\n",
    "        # if name is in the name_mismatches dictionary, search for the replacement name\n",
    "        if (atom.resname + ' ' + atom.name) in name_mismatches.keys():\n",
    "            #print('mismatched atom name: ', atom.name, 'in residue: ', atom.resname, 'replacing with: ', name_mismatches[(atom.resname + ' ' + atom.name)])\n",
    "            atom_name_template = name_mismatches[(atom.resname + ' ' + atom.name)]\n",
    "\n",
    "        # match atom with same name and residue ID in the structure universe (from AF ensemble)\n",
    "        atom_name_structure = u_structure.atoms[(u_structure.atoms.names == atom_name_template) & (u_structure.atoms.resids == atom.resid) ].names\n",
    "\n",
    "        # if the atom is found in the structure universe, replace the coordinates\n",
    "        if len(atom_name_structure) > 0:\n",
    "\n",
    "            # replace the coordinates of the atom in the template universe with the coordinates of the atom in the structure universe\n",
    "            atom.position = u_structure.atoms[(u_structure.atoms.names == atom_name_template) & (u_structure.atoms.resids == atom.resid) ].positions\n",
    "\n",
    "            # append the atom to a list of atoms\n",
    "            modified_atoms.append((str(atom.resid) + str(atom.name)))\n",
    "\n",
    "        else:\n",
    "            print('ERROR: with: ', atom.name, atom.resname)\n",
    "\n",
    "            # add this to a list of resname atom name pairs that are not found in the structure universe\n",
    "            error_type_list.append((atom.resname, atom.name))\n",
    "\n",
    "            error_counter += 1\n",
    "            # handle mismatching atoms \n",
    "\n",
    "            # check that the residue itself is present\n",
    "            if atom.resid in u_structure.atoms.resids:              # if the resid for this missing atom is in the structure universe\n",
    "\n",
    "                pass\n",
    "                #print('no atom in resid', atom.resid, atom.resname, 'with name: ', atom.name, 'found')\n",
    "            \n",
    "                #    # get the average position of the atoms in the structure universe with the same resid\n",
    "                #    incomplete_resid_average_position_structure = np.mean(u_structure.atoms[u_structure.atoms.resids == atom.resid].positions, axis=0)\n",
    "                #    # get the average position of the atoms in the template universe with the same resid\n",
    "                #    incomplete_resid_average_position_template = np.mean(u_template.atoms[u_template.atoms.resids == atom.resid].positions, axis=0) \n",
    "                #    # get the vector between the two averages\n",
    "                #    fudge_position_vector = incomplete_resid_average_position_structure - incomplete_resid_average_position_template\n",
    "                #    # add the vector to the atom position in the template universe\n",
    "                #    atom.position = atom.position + fudge_position_vector\n",
    "\n",
    "            else:\n",
    "                print('ERROR: resid not found in structure universe: ', atom.resid)\n",
    "\n",
    "    print('number of atoms not found in structure universe: ', error_counter)\n",
    "    # print unique error types in the list\n",
    "    display(set(error_type_list))\n",
    "    return u_template.atoms.positions # not important becauase the universe is modified in place, just return something\n",
    "\n",
    "# turns your mda selection token into a safe list of atom IDs to pass to PLUMED\n",
    "# because MOLINFO may or may not work depending on what python interpreter\n",
    "# you have available at runtime\n",
    "def selection_parser(mda_selection):\n",
    "    selection_string = ''\n",
    "    for i in u_template.select_atoms(mda_selection).atoms.ids:\n",
    "        selection_string += str(i) + ','\n",
    "    selection_string = selection_string[:-1]\n",
    "    return selection_string\n",
    "\n",
    "# write a plumed input file for REUS, if your CV is not a distance then\n",
    "# you will need to modify this script appropriately\n",
    "def plumed_input_writer(window_values, plumed_file, force_constant=1000):\n",
    "\n",
    "    # convert window values to nm (from Angstrom)\n",
    "    window_values = window_values / 10\n",
    "\n",
    "    with open(plumed_file, 'w') as f:\n",
    "        f.write('MOLINFO STRUCTURE=%s \\n\\n' % template_pdb_for_umbrella_sampling) #window_0/window_0.pdb\n",
    "        if pcavar == True:\n",
    "            f.write('CV: PCAVARS REFERENCE=%s TYPE=OPTIMAL\\n\\n' % pca_ref_pdb) #{@mda:{}}\n",
    "        else:\n",
    "            f.write('com1: CENTER ATOMS=%s \\n' % selection_parser(com1)) #{@mda:{}}\n",
    "            f.write('com2: CENTER ATOMS=%s \\n' % selection_parser(com2)) #{@mda:{}}\n",
    "            f.write('\\n')\n",
    "            f.write('CV: DISTANCE ATOMS=com1,com2 \\n\\n')\n",
    "        f.write('umbrella_restraint: RESTRAINT ARG=CV KAPPA=%s AT=@replicas:' % force_constant)\n",
    "        for i in window_values:\n",
    "            # should be no comma after the last value\n",
    "            if i == window_values[-1]:\n",
    "                f.write(str(i.round(4)) + '\\n')\n",
    "            else:\n",
    "                f.write(str(i.round(4)) + ',')\n",
    "        f.write('\\nPRINT ARG=CV,umbrella_restraint.* FILE=COLVAR_MULTI\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "# prepare directories for each window\n",
    "umbrella_sampling_directory = base_directory + 'umbrella_sampling/'\n",
    "window_directories = []\n",
    "for i in range(mc_n_bins):\n",
    "    window_directories.append(umbrella_sampling_directory + 'window_' + str(i))\n",
    "    if not os.path.exists(window_directories[i]):\n",
    "        os.makedirs(window_directories[i])\n",
    "    shutil.copy(structure_directory + mc_runs_df.head(1)['path structures'].values[0][i], window_directories[i])    # copy the relevant structure in mc_runs_df to the window directory\n",
    "\n",
    "# for each window, replace the coordinates of the protein in the template universe with the coordinates of the structure at the corresponding index in the best path\n",
    "for i in range(mc_n_bins):\n",
    "\n",
    "    # make a copy of the template universe\n",
    "    u = u_template.copy()\n",
    "\n",
    "    # make a universe out of the structure corresponding to the index in the best path\n",
    "    structure = mc_runs_df.head(1)['path structures'].values[0][i]\n",
    "    u_structure = mda.Universe(structure_directory + structure, structure_directory + structure)\n",
    "\n",
    "    # align the structure to the template universe\n",
    "    u_structure = align_universe(u_structure, u)\n",
    "    new_coordinates = protein_coordinate_replacer(u_structure, u)\n",
    "\n",
    "    # write out the universe to a pdb file\n",
    "    u.atoms.write(window_directories[i] + '/window_' + str(i) + '.pdb')\n",
    "\n",
    "# write out the plumed input file\n",
    "plumed_input_writer(ideal_window_values, umbrella_sampling_directory + '/plumed.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propka\n",
    "from propkatraj import PropkaTraj\n",
    "\n",
    "residues = ['205', '305', '332', '346', '273', '280', '335'] # residues of interest, must appear in the propkatraj output dataframe\n",
    "pka_df = ensemble_df      # copy dataframe \n",
    "\n",
    "# results array\n",
    "pka_results = np.zeros((len(structures), len(residues)))\n",
    "for structure in structures[1:]:\n",
    "    \n",
    "    # make a universe\n",
    "    u = mda.Universe(structure_directory + structure, structure_directory + structure)\n",
    "    u.atoms.residues.resids += resid_offset\n",
    "    u.atoms.segments.segids = 'A'\n",
    "    u.atoms.chainIDs = 'A'\n",
    "\n",
    "    # run propka \n",
    "    pkatraj = PropkaTraj(u, select='protein', skip_failure=True, Verbose=True)\n",
    "    pkatraj.run() # the dataframe is pkatraj.results.pkas\n",
    "    \n",
    "    # store results\n",
    "    for resid in residues:\n",
    "        try:\n",
    "          pka_results[structures.index(structure), residues.index(resid)] = pkatraj.results.pkas[int(resid)]\n",
    "        except KeyError:\n",
    "          pka_results[structures.index(structure), residues.index(resid)] = np.nan\n",
    "\n",
    "# make a dataframe\n",
    "pka_df = pd.DataFrame(pka_results, columns=residues, index=structures)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the pKas\n",
    "\n",
    "def plot_ensemble_pka(resid, pH, pH_span=3):\n",
    "\n",
    "    plt.hexbin(ensemble_df_pka[plot_variable_1], ensemble_df_pka[plot_variable_2], C=ensemble_df_pka['pka_' + resid], cmap='coolwarm_r', gridsize=30, reduce_C_function=np.nanmean)\n",
    "    plt.xlabel(plot_variable_1)\n",
    "    plt.ylabel(plot_variable_2)\n",
    "\n",
    "    plt.colorbar(label='pKa')\n",
    "    plt.clim(pH-pH_span, pH+pH_span)\n",
    "\n",
    "    ## labels\n",
    "    plt.title('AF ensemble: pKa resID ' + resid)\n",
    "    plt.annotate('IF',  (0.20, 0.95), xycoords='axes fraction', ha='left',  va='top', alpha=1)\n",
    "    plt.annotate('OF',  (0.95, 0.20), xycoords='axes fraction', ha='right', va='top', alpha=1)\n",
    "    plt.annotate('OCC', (0.30, 0.25), xycoords='axes fraction', ha='left',  va='top', alpha=1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# make a copy of the ensemble_df dataframe with the pka values added\n",
    "ensemble_df_pka = ensemble_df.copy()\n",
    "for resid in residues:\n",
    "    ensemble_df_pka['pka_' + resid] = pka_df[resid].values\n",
    "ensemble_df_pka = ensemble_df_pka.replace(0, np.nan)    # set all zeros to nan\n",
    "\n",
    "luminal_residues = ['205', '332', '335', '273']\n",
    "cyto_residues = ['305', '346', '280']\n",
    "\n",
    "for resid in luminal_residues:\n",
    "    plot_ensemble_pka(resid, pH=5)\n",
    "\n",
    "for resid in cyto_residues:\n",
    "    plot_ensemble_pka(resid, pH=7)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
